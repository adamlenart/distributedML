{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW6.7  Implement Bernoulli Mixture Model via EM\n",
    "Implement the EM clustering algorithm to determine Bernoulli Mixture Model for discrete data in MRJob.\n",
    "\n",
    "As a unit test use the dataset in the following slides:\n",
    "\n",
    "https://www.dropbox.com/s/maoj9jidxj1xf5l/MIDS-Live-Lecture-06-EM-Bernouilli-MM-Systems-Test.pdf?dl=0\n",
    "\n",
    "Cross-check that you get the same cluster assignments and cluster Bernouilli models as presented in the slides after 25 iterations. Dont forget the smoothing.\n",
    "\n",
    "As a full test: use the same dataset from HW 4.5, the Tweet Dataset. \n",
    "Using this data, you will implement a 1000-dimensional EM-based Bernoulli Mixture Model  algorithm in MrJob on the users\n",
    "by their 1000-dimensional word stripes/vectors using K = 4.  Use the same smoothing as in the unit test.\n",
    "\n",
    "Repeat this experiment using your KMeans MRJob implementation fron HW4.\n",
    "Report the rand index score using the class code as ground truth label for both algorithms and comment on your findings.\n",
    "\n",
    "Here is some more information on the Tweet Dataset.\n",
    "\n",
    "Here you will use a different dataset consisting of word-frequency distributions \n",
    "for 1,000 Twitter users. These Twitter users use language in very different ways,\n",
    "and were classified by hand according to the criteria:\n",
    "\n",
    "0: Human, where only basic human-human communication is observed.\n",
    "\n",
    "1: Cyborg, where language is primarily borrowed from other sources\n",
    "(e.g., jobs listings, classifieds postings, advertisements, etc...).\n",
    "\n",
    "2: Robot, where language is formulaically derived from unrelated sources\n",
    "(e.g., weather/seismology, police/fire event logs, etc...).\n",
    "\n",
    "3: Spammer, where language is replicated to high multiplicity\n",
    "(e.g., celebrity obsessions, personal promotion, etc... )\n",
    "\n",
    "Check out the preprints of  recent research,\n",
    "which spawned this dataset:\n",
    "\n",
    "http://arxiv.org/abs/1505.04342\n",
    "http://arxiv.org/abs/1508.01843\n",
    "\n",
    "The main data lie in the accompanying file:\n",
    "\n",
    "[topUsers_Apr-Jul_2014_1000-words.txt](#https://www.dropbox.com/s/6129k2urvbvobkr/topUsers_Apr-Jul_2014_1000-words.txt?dl=0)\n",
    "\n",
    "and are of the form:\n",
    "\n",
    "USERID,CODE,TOTAL,WORD1_COUNT,WORD2_COUNT,...\n",
    ".\n",
    ".\n",
    "\n",
    "where\n",
    "\n",
    "USERID = unique user identifier\n",
    "CODE = 0/1/2/3 class code\n",
    "TOTAL = sum of the word counts\n",
    "\n",
    "Using this data, you will implement a 1000-dimensional K-means algorithm in MrJob on the users\n",
    "by their 1000-dimensional word stripes/vectors using several \n",
    "centroid initializations and values of K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete likelihood\n",
    "\n",
    "Let $N$ be the number of documents of $M$ unique terms of $x_{n,m} = (x_{1,1}\\ldots,x_{N,M})$ binary features denoting whether a word is present in a document, that is $x_{n,m} \\in {0,1}$. Moreover, let $\\mu_{k,m}$ denote the probability that word $x_{n,m}$ is in cluster $k$ ,$k \\in (1,\\ldots,K$), and $\\pi_k$, $\\sum_k \\pi = 1$, the proportion of documents in cluster $k$.\n",
    "\n",
    "Hence, the probability of a word belonging to cluster $k$ can be written as\n",
    "\n",
    "\\begin{equation}\n",
    "  \\text{Pr}(x_{n,m} \\mid \\mu_{k,m}) = \\mu_{k,m}^{x_{n,m}}\\left(1-\\mu_{k,m}\\right)^{1-x_{n,m}}\n",
    "\\end{equation}\n",
    "\n",
    "and the joint probability of all words being in cluster $k$ is (by assuming independence between their presences)\n",
    "\n",
    "\\begin{equation}\n",
    "   \\text{Pr}(\\mathbf{x} \\mid \\mathbf{\\mu}_{k}) = \\prod_{m = 1}^M \\mu_{k,m}^{x_{n,m}}\\left(1-\\mu_{k,m}\\right)^{1-x_{n,m}}\\,.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The documents are assigned to clusters through the set of words that they contain, that is $x_n \\in d$ for document $d$. $d$ However, the assignment so far has been probabilistic, and is performed through a latent vector of $K$-dimensional binary representations $\\mathbf{r}_n$ which equal $1$ if document is in cluster $k$, else 0. Let the set of $N$ representations denoted by $\\mathbf{R}$, then \n",
    "\n",
    "\\begin{equation}\n",
    "   \\text{Pr}(\\mathbf{R} \\mid \\mathbf{\\pi}) = \\prod_{n=1}^N \\prod_{k=1}^{K} \\pi_{k}^{r_{n,k}}\n",
    "\\end{equation}\n",
    "\n",
    "For the set of all documents $\\mathbf{D} = (d_1,\\ldots,d_N)$, the likelihood that the distribution of features across all documents were generated by the representations, the probability that a given word is in a given cluster and the proportion of documents in clusters,\n",
    "\n",
    "\\begin{equation}\n",
    "  \\text{Pr}(\\mathbf{D} \\mid \\mathbf{R},\\mathbf{\\mu},\\mathbf{\\pi}) = \\prod_{n=1}^N \\prod_{k=1}^K \\left(\\prod_{m=1}^M \\mu_{k,m}^{x_{n,m}}(1-\\mu_{k,m})^{1-x_{n,m}}\\right)^{r_{n,k}}\n",
    "\\end{equation}\n",
    "\n",
    "and the complete data log-likelihood, that is that the observed distribution of words across documents and their representations are generated by $(\\mathbf{\\mu},\\mathbf{\\pi})$:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\text{Pr}(\\mathbf{D}, \\mathbf{R} \\mid \\mathbf{\\mu},\\mathbf{\\pi}) = \\text{Pr}(\\mathbf{D} \\mid \\mathbf{R},\\mathbf{\\mu},\\mathbf{\\pi}) \\text{Pr}(\\mathbf{R} \\mid \\mathbf{\\pi}) = \\prod_{n=1}^N \\prod_{k=1}^K \\left(\\pi_k\\prod_{m=1}^M \\mu_{k,m}^{x_{n,m}}(1-\\mu_{k,m})^{1-x_{n,m}}\\right)^{r_{n,k}}\\,.\n",
    "\\end{equation}\n",
    "\n",
    "### EM algorithm: E step\n",
    "\n",
    "However, $\\mathbf{R}$ is unobserved but can be approximated as its expectation,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{E}[r_{n,k}] = \\sum_{n=1}^N \\text{Pr}(r_{n,k} \\mid \\mathbf{x}_n, \\mathbf{\\mu}, \\mathbf{\\pi})r_{n,k} = \\frac{\\pi_k \\text{Pr}(\\mathbf{x}_n \\mid \\mathbf{\\mu}_k)}{\\sum_{\\kappa=1}^K\\pi_\\kappa \\text{Pr}(\\mathbf{x}_n \\mid \\mathbf{\\mu}_\\kappa)} = \\frac{\\pi_k \\prod_{m = 1}^M \\mu_{k,m}^{x_{n,m}}\\left(1-\\mu_{k,m}\\right)^{1-x_{n,m}}}{\\sum_{\\kappa =1}^K \\pi_\\kappa \\prod_{m = 1}^M \\mu_{\\kappa,m}^{x_{n,m}}\\left(1-\\mu_{\\kappa,n}\\right)^{1-x_{n,m}}}\n",
    "\\end{equation} \n",
    "\n",
    "### EM algorithm: M step\n",
    "\n",
    "Taking the natural logarithm of $\\mathcal{L}$ and substituting $\\text{E}[r_{n,k}]$ for $r_{n,k}$ yield\n",
    "\n",
    "\\begin{equation}\n",
    "  \\mathcal{l} = \\sum_{n=1}^N\\sum_{k=1}^K \\text{E}[r_{n,k}]\\left(\\log \\pi_k + \\sum_{m=1}^M x_{n,m}\\log \\mu_{k,m}+(1-x_{n,m})\\log (1-\\mu_{k,m})\\right)\\,.\n",
    "\\end{equation}\n",
    "\n",
    "Taking the partial derivative of $l$ by $\\mathbf{\\mu}$ and equating it to 0 gives after some arithmetic\n",
    "\n",
    "\\begin{equation}\n",
    "  \\mu_{k,m}^{max} = \\frac{\\sum_{n=1}^N \\mathbf{x}_n\\text{E}[r_{n,k}]}{\\sum_{n=1}^N \\text{E}[r_{n,k}]}\\,.\n",
    "\\end{equation}\n",
    "\n",
    "To calculate $\\pi_k^{max}$, we can use Lagrange-multipliers because of the $\\sum_k \\pi_k = 1$ constraint, after derivating the Lagrangian by $\\pi_k$ and the introduced multiplier and solving the systems of equations, we get\n",
    "\n",
    "\\begin{equation}\n",
    "  \\pi_k^{max} = \\frac{\\sum_{n=1}^N \\text{E}[r_{n,k}]}{N} \n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = [\n",
    "\"hot chocolate cocoa beans\",\n",
    "\"cocoa ghana africa\",\n",
    "\"beans harvest ghana\",\n",
    "\"cocoa butter\",\n",
    "\"butter truffles\",\n",
    "\"sweet chocolate\",\n",
    "\"sweet sugar\",\n",
    "\"sugar cane brazil\",\n",
    "\"sweet sugar beet\",\n",
    "\"sweet cake icing\",\n",
    "\"cake black frost\"\n",
    "]\n",
    "\n",
    "init_rs = [\n",
    "    \"None None\",\n",
    "    \"None None\",\n",
    "    \"None None\",\n",
    "    \"None None\",\n",
    "    \"None None\",\n",
    "    \"1     0\",\n",
    "    \"0     1\",\n",
    "    \"None None\",\n",
    "    \"None None\",\n",
    "    \"None None\",\n",
    "    \"None None\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to either append a key column or add $r_{nk}$ initialization to the data file or use a single mapper. Otherwise, it is difficult to ensure that the order of rows of the $r_{nk}$ matrix will be the same as the mapped data with multiple mappers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('unit_test_67.txt','w') as outfile:\n",
    "    for document, init in zip(documents, init_rs):\n",
    "        outfile.write(\"{0},{1}\\n\".format(document,init))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile bernoulli_init.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MRBernoulliMixtureModelInit(MRJob):\n",
    "        \n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MRBernoulliMixtureModelInit, self).__init__(*args, **kwargs)\n",
    "        self.pi = [1.0/self.options.k]*self.options.k\n",
    "        self.doc_counter = 0\n",
    "        # write an empty file to disk to fill up later with vocabulary entries\n",
    "        open('vocabulary.txt','w').close()\n",
    "\n",
    "    def configure_options(self):\n",
    "        super(MRBernoulliMixtureModelInit,self).configure_options()\n",
    "        self.add_passthrough_option('--smooth', type = 'float', default = 0)\n",
    "        self.add_passthrough_option('--k', type='int', default=2)\n",
    "\n",
    "    def steps(self):\n",
    "\n",
    "        # 1st job: build binary feature represenations\n",
    "        # 2nd job: run E-M algorithm\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper_init = self.set_vocabulary,\n",
    "                mapper = self.get_vocabulary,\n",
    "                mapper_final = self.save_vocabulary,\n",
    "                reducer_init = self.read_vocabulary,\n",
    "                reducer = self.binarize_doc_features,\n",
    "                reducer_final = self.save_binary_features\n",
    "            ),\n",
    "            MRStep(\n",
    "                reducer_init = self.store_reducer_parameters,\n",
    "                reducer = self.M_aggregate,\n",
    "                reducer_final = self.M_calculate\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    # ----------------------------------- helper functions -------------------------------- #\n",
    "\n",
    "    def maximize_mu_m(self,r_nk,x_nm):\n",
    "        # weighted mean of number of documents in cluster k: column sum of r_nk*x_n\n",
    "        weighted_mean_num_docs = np.dot(r_nk,x_nm)+self.options.smooth\n",
    "        # effective number of documents in cluster k: column sum of r_nk\n",
    "        num_docs = np.sum(r_nk,axis=0)+self.options.smooth*2\n",
    "        return weighted_mean_num_docs/num_docs\n",
    "\n",
    "    def maximize_pi(self,r_nk):\n",
    "        # number of documents\n",
    "        N = r_nk.shape[0]\n",
    "        # effective number of documents in cluster k: column sum of r_nk\n",
    "        num_docs = np.sum(r_nk, axis=0)\n",
    "        return num_docs/N\n",
    "\n",
    "    # --------------------------------------- MR steps ------------------------------------ #\n",
    "\n",
    "    def set_vocabulary(self):\n",
    "        self.vocabulary = []\n",
    "        self.num_words_voc = 0\n",
    "\n",
    "\n",
    "    def vocabulary_builder(self,word):\n",
    "        if word not in self.vocabulary:\n",
    "            self.vocabulary.append(word)\n",
    "\n",
    "\n",
    "    def get_vocabulary(self, _, value):\n",
    "        word_list, r_nks = value.split(\",\")\n",
    "        words = word_list.split()\n",
    "        for word in words:\n",
    "            self.vocabulary_builder(word)\n",
    "        yield np.random.uniform(size=1), (words, r_nks)\n",
    "\n",
    "\n",
    "    def save_vocabulary(self):\n",
    "        with open('/home/cloudera/PycharmProjects/HW6/vocabulary.txt','a') as outfile:\n",
    "            for item in self.vocabulary:\n",
    "                outfile.write(\"%s\\n\" % item)\n",
    "\n",
    "    def read_vocabulary(self):\n",
    "        self.vocabulary = list(set(line.strip() for line in open('/home/cloudera/PycharmProjects/HW6/vocabulary.txt','r')))\n",
    "        self.bin_feat = [None]*len(self.vocabulary)\n",
    "            \n",
    "    def binarize_doc_features(self, key, values):\n",
    "        for value in values:\n",
    "            features, r_nks = value\n",
    "            # build feature representations\n",
    "            binary_features = [0]*len(self.vocabulary)\n",
    "            insert_index = [self.vocabulary.index(str(word)) for word in features]\n",
    "            for index in insert_index:\n",
    "                binary_features[index] = 1\n",
    "            # pass to self.save binary features()     \n",
    "            self.bin_feat = np.vstack((self.bin_feat,binary_features))\n",
    "            yield key, (binary_features, r_nks)\n",
    "   \n",
    "    def save_binary_features(self):        \n",
    "        with open('/home/cloudera/PycharmProjects/HW6/binary_features.txt','a') as outfile:\n",
    "            for line in self.bin_feat:\n",
    "                if line[0] is not None:\n",
    "                    outfile.write(\"%s\\n\" % line)\n",
    "\n",
    "\n",
    "    def store_reducer_parameters(self):\n",
    "        self.vocabulary = list(set(line.strip() for line in open('/home/cloudera/PycharmProjects/HW6/vocabulary.txt', 'r')))\n",
    "        self.vocabulary_size = len(self.vocabulary)\n",
    "        self.r_nk = []\n",
    "        self.x_nm = [None]*self.vocabulary_size\n",
    "\n",
    "    def M_aggregate(self, _,values):\n",
    "        for value in values:\n",
    "            x_nms, r_nks = value\n",
    "            r_nks = r_nks.split()\n",
    "            r_nks = [float(r) for r in r_nks if r != 'None']\n",
    "            # emit each cluster separately\n",
    "            for k in range(len(r_nks)):\n",
    "#                 # extract r_nk and x_nm\n",
    "                 self.r_nk.append(r_nks[k])\n",
    "                 self.x_nm = np.vstack((self.x_nm, np.array(x_nms)))\n",
    "        \n",
    "    def M_calculate(self):\n",
    "        # delete the first line of None's which was created as a placeholder\n",
    "        self.x_nm = np.delete(self.x_nm,0,0)\n",
    "        # maximize mu and pi\n",
    "        mu = self.maximize_mu_m(np.array(self.r_nk),self.x_nm)\n",
    "        pi = self.maximize_pi(np.array(self.r_nk))\n",
    "        yield None,(pi,mu)\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRBernoulliMixtureModelInit.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm /home/cloudera/PycharmProjects/HW6/vocabulary.txt\n",
    "!rm /home/cloudera/PycharmProjects/HW6/binary_features.txt\n",
    "!python bernoulli_init.py unit_test_67.txt > 'bmm_it.txt'\n",
    "!cat 'bmm_it.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile bernoulli_update.py\n",
    "#!/home/cloudera/anaconda2/bin/python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "class MRBernoulliMixtureModelUpdate(MRJob):\n",
    "        \n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MRBernoulliMixtureModelUpdate, self).__init__(*args, **kwargs)\n",
    "        self.pi = [1.0/self.options.k]*self.options.k\n",
    "        self.doc_counter = 0\n",
    "        # write an empty file to disk to fill up later with vocabulary entries\n",
    "        open('vocabulary.txt','w').close()\n",
    "\n",
    "    def configure_options(self):\n",
    "        super(MRBernoulliMixtureModelUpdate,self).configure_options()\n",
    "        self.add_passthrough_option('--smooth', type = 'float', default = 0)\n",
    "        self.add_passthrough_option('--k', type='int', default=2)\n",
    "\n",
    "    def steps(self):\n",
    "\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper = self.load_binary_features,\n",
    "                ),\n",
    "            MRStep(\n",
    "                mapper_init = self.read_vocabulary,\n",
    "                mapper = self.E,\n",
    "                reducer_init = self.store_reducer_parameters,\n",
    "                reducer = self.M_aggregate,\n",
    "                reducer_final = self.M_calculate,\n",
    "#                jobconf = {'mapred.map.tasks':'1'},\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    # ----------------------------------- helper functions -------------------------------- #\n",
    "\n",
    "    def log_prob_all_word_cluster_k(self,x_n,mu_k):\n",
    "        '''Calculates the probability that all binarized representations are in cluster k.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x_n = a list of binary representaitons of words in document n (length M)\n",
    "        mu_k = list of probabilities of mth words in cluster k (length M)\n",
    "        '''\n",
    "        \n",
    "        PR = 0\n",
    "        try:\n",
    "            PR += x_n*math.log(mu_k)  + (1-x_n)*math.log(1-mu_k)\n",
    "        except ValueError:\n",
    "            PR += x_n*math.log(mu_k+1e-10) + (1 - x_n)*math.log(1-mu_k+1e-10)\n",
    "        return PR\n",
    "    \n",
    "\n",
    "    def maximize_mu_m(self,r_nk,x_nm):\n",
    "        # weighted mean of number of documents in cluster k: column sum of r_nk*x_n\n",
    "        weighted_mean_num_docs = np.dot(x_nm.T,r_nk)+self.options.smooth\n",
    "        # effective number of documents in cluster k: column sum of r_nk\n",
    "        num_docs = np.sum(r_nk,axis=0)+self.options.smooth*2\n",
    "        return weighted_mean_num_docs/num_docs\n",
    "\n",
    "    def maximize_pi(self,r_nk):\n",
    "        # number of documents\n",
    "        N = r_nk.shape[0]\n",
    "        # effective number of documents in cluster k: column sum of r_nk\n",
    "        num_docs = np.sum(r_nk, axis=0)\n",
    "        return num_docs/N\n",
    "\n",
    "    # --------------------------------------- MR steps ------------------------------------ #\n",
    "    \n",
    "    def load_binary_features(self, _, x_ns):\n",
    "        \n",
    "        x_n = x_ns.strip(\"[,] \")\n",
    "        x_n = [int(x) for x in x_n if x != \" \"]\n",
    "        yield None, x_n\n",
    "\n",
    "    def read_vocabulary(self):\n",
    "        self.vocabulary = list(set(line.strip() for line in open('/home/cloudera/PycharmProjects/HW6/vocabulary.txt','r')))\n",
    "        self.pars = [line.strip().split() for line in open('/home/cloudera/W261/Spring2017/HW6/bmm_it.txt','r')]\n",
    "        self.pi = []\n",
    "        self.mu = [None]*len(self.vocabulary)\n",
    "        for pars in self.pars:\n",
    "            pars = eval(pars[1])\n",
    "            self.pi.append(float(pars[0]))\n",
    "            foo = [float(mu) for mu in pars[1]]\n",
    "            self.mu = np.vstack((self.mu,pars[1]))\n",
    "        self.mu = np.delete(self.mu,0,0)\n",
    "                \n",
    "            \n",
    "    def E(self, _, x_n):\n",
    "        numerator = []\n",
    "        for k in range(len(self.pi)):\n",
    "            mu_k = self.mu[k,]\n",
    "            pi_k = self.pi[k]\n",
    "            # calculate the numerator of E[r_{n,k}] for each k\n",
    "            PR = 0\n",
    "            for x_nm, mu_km in zip(x_n, mu_k):\n",
    "                PR += self.log_prob_all_word_cluster_k(x_nm,mu_km)\n",
    "            numerator.append(pi_k*np.exp(PR))\n",
    "        denominator = np.sum(numerator)\n",
    "        r_nk = numerator/denominator\n",
    "        yield _, (x_n, r_nk)\n",
    "   \n",
    "\n",
    "    def store_reducer_parameters(self):\n",
    "        self.i = 0\n",
    "        self.vocabulary = list(set(line.strip() for line in open('/home/cloudera/PycharmProjects/HW6/vocabulary.txt', 'r')))\n",
    "        self.vocabulary_size = len(self.vocabulary)\n",
    "        self.r_nk = [None]*self.options.k\n",
    "        self.x_nm = [None]*self.vocabulary_size\n",
    "\n",
    "    def M_aggregate(self, _,values):\n",
    "        for value in values:\n",
    "            x_nms, r_nks = value\n",
    "            r_nks = [float(r) for r in r_nks if r != 'None']\n",
    "            self.r_nk = np.vstack((self.r_nk,np.array(r_nks)))\n",
    "            self.x_nm = np.vstack((self.x_nm, np.array(x_nms)))\n",
    "                \n",
    "#            yield self.r_nk, self.x_nm\n",
    "        \n",
    "    def M_calculate(self):\n",
    "        # delete the first line of None's which was created as a placeholder\n",
    "        self.x_nm = np.delete(self.x_nm,0,0)\n",
    "        self.r_nk = np.delete(self.r_nk,0,0)\n",
    "        # maximize mu and pi\n",
    "        mu = self.maximize_mu_m(np.array(self.r_nk),self.x_nm)\n",
    "        pi = self.maximize_pi(np.array(self.r_nk))\n",
    "        for p, m in zip(pi, mu.T):\n",
    "            yield None, (p,m)\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRBernoulliMixtureModelUpdate.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python bernoulli_update.py /home/cloudera/PycharmProjects/HW6/binary_features.txt --k 2 --smooth 0.0001  > 'bmm_it_temp.txt'\n",
    "!mv bmm_it_temp.txt bmm_it.txt\n",
    "!cat bmm_it.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile bmm_driver.sh\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "rm /home/cloudera/PycharmProjects/HW6/vocabulary.txt\n",
    "rm /home/cloudera/PycharmProjects/HW6/binary_features.txt\n",
    "\n",
    "# script creates 'vocabulary.txt', 'binary_features.txt' and saves the optimal parameters in 'bmm_it.txt'\n",
    "python bernoulli_init.py unit_test_67.txt -q > 'bmm_it.txt'\n",
    "\n",
    "\n",
    "for i in {0..24}\n",
    "    do\n",
    "       echo \"iteration: $i\"\n",
    "       time python bernoulli_update.py /home/cloudera/PycharmProjects/HW6/binary_features.txt \\\n",
    "       --k 2 --smooth 0.0001 -q > 'bmm_it_temp.txt'\n",
    "       mv bmm_it_temp.txt bmm_it.txt \n",
    "       echo '\n",
    "        ready\n",
    "        '\n",
    "done\n",
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x bmm_driver.sh\n",
    "!./bmm_driver.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "words = ['ghana', 'butter',  'cane',  'icing', 'truffles', 'sweet', 'frost', 'brazil', 'africa', 'chocolate', 'cocoa',  \n",
    " 'hot', 'beans', 'cake', 'beet', 'sugar', 'harvest', 'black']\n",
    "pi = []\n",
    "mu = [None]*len(words)\n",
    "\n",
    "with open('bmm_it.txt','r') as infile:\n",
    "    for line in infile:\n",
    "        pars = eval(line.split()[1])\n",
    "        pi.append(round(pars[0],2))\n",
    "        foo = [round(m,3) for m in pars[1]]\n",
    "        mu = np.vstack((mu,foo))\n",
    "\n",
    "for i in range(mu.shape[1]): \n",
    "    if i == 0:\n",
    "        print '{word:10} | {p1:5} | {p2:5} |'.format(word='prop.',p1=pi[0],p2=pi[1])  \n",
    "        print '-'*30\n",
    "    print '{word:10} | {k1:5} | {k2:5} |'.format(word=words[i],k1=mu[1:,i][0],k2=mu[2:,i][0])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python2]",
   "language": "python",
   "name": "conda-env-python2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
