{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# K-means clusteriing on Hadoop with MRJob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess log file data\n",
    "\n",
    "\n",
    "This dataset captures which areas (Vroots) of www.microsoft.com each user visited in a one-week timeframe in Feburary 1998.\n",
    "\n",
    "\n",
    "#### Data Format\n",
    "<PRE>\n",
    "The data is in an ASCII-based sparse-data format called \"DST\". Each line of the data file starts with a letter which tells the line's type. The three line types of interest are:\n",
    "-- Attribute lines:\n",
    "For example, 'A,1277,1,\"NetShow for PowerPoint\",\"/stream\"'\n",
    "Where:\n",
    "  'A' marks this as an attribute line, \n",
    "  '1277' is the attribute ID number for an area of the website (called a Vroot),\n",
    "  '1' may be ignored, \n",
    "  '\"NetShow for PowerPoint\"' is the title of the Vroot, \n",
    "  '\"/stream\"' is the URL relative to \"http://www.microsoft.com\"\n",
    "\n",
    "Case and Vote Lines:\n",
    "For each user, there is a case line followed by zero or more vote lines.\n",
    "For example:\n",
    "  C,\"10164\",10164\n",
    "  V,1123,1\n",
    "  V,1009,1\n",
    "  V,1052,1\n",
    "Where:\n",
    "  'C' marks this as a case line, \n",
    "  '10164' is the case ID number of a user, \n",
    "  'V' marks the vote lines for this case, \n",
    "  '1123', 1009', 1052' are the attributes ID's of Vroots that a user visited. \n",
    "  '1' may be ignored.\n",
    "</PRE>\n",
    "---\n",
    " Here, you must transform/preprocess the data on a single node (i.e., not on a cluster of nodes) from the following format:\n",
    "\n",
    "- C,\"10001\",10001   #Visitor id 10001\n",
    "- V,1000,1          #Visit by Visitor 10001 to page id 1000\n",
    "- V,1001,1          #Visit by Visitor 10001 to page id 1001\n",
    "- V,1002,1          #Visit by Visitor 10001 to page id 1002\n",
    "- C,\"10002\",10002   #Visitor id 10001\n",
    "- V\n",
    "- Note: #denotes comments\n",
    "\n",
    "\n",
    "to the following format (V, PageID, 1, C, Visitor):\n",
    "\n",
    "- V,1000,1,C, 10001\n",
    "- V,1001,1,C, 10001\n",
    "- V,1002,1,C, 10001\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from csv import reader, writer\n",
    "\n",
    "# set output\n",
    "outfile = open('anonymous-msweb-preprocessed.data', 'w')\n",
    "webdata_preprocessed = writer(outfile)\n",
    "\n",
    "# read in data and preprocess\n",
    "with open('/home/cloudera/W261/week4/anonymous-msweb.data', 'r') as infile:\n",
    "    webdata = reader(infile)\n",
    "    for row in webdata:\n",
    "        # If Case line\n",
    "        if row[0]=='C':\n",
    "            # save case id\n",
    "            Visitor = row[1]\n",
    "        #  if Vote line\n",
    "        elif row[0]=='V':\n",
    "            # save page id\n",
    "            PageID = row[1]\n",
    "            # output in the format of (V, PageID, 1, C, Visitor)\n",
    "            webdata_preprocessed.writerow(['V',PageID,1,'C',Visitor])\n",
    "        # do not print if any other line type\n",
    "        else:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V,1000,1,C,10001\n",
      "V,1001,1,C,10001\n",
      "V,1002,1,C,10001\n",
      "V,1001,1,C,10002\n",
      "V,1003,1,C,10002\n",
      "V,1001,1,C,10003\n",
      "V,1003,1,C,10003\n",
      "V,1004,1,C,10003\n",
      "V,1005,1,C,10004\n",
      "V,1006,1,C,10005\n",
      "98531 anonymous-msweb-preprocessed.data\n"
     ]
    }
   ],
   "source": [
    "!head -10 anonymous-msweb-preprocessed.data\n",
    "!wc -l anonymous-msweb-preprocessed.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the attributes also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from csv import reader, writer\n",
    "\n",
    "# set output (write in binary mode otherwise, I can't fathom why, the write is not complete but stops at page id 1036)\n",
    "outfile = open('anonymous-msweb-attributes.data', 'wb')\n",
    "webdata_preprocessed = writer(outfile)\n",
    "\n",
    "# read in data and preprocess\n",
    "with open('/home/cloudera/W261/week4/anonymous-msweb.data', 'r') as infile:\n",
    "    webdata = reader(infile)\n",
    "    for row in webdata:\n",
    "        # If Case line\n",
    "        if row[0]=='A':\n",
    "            webdata_preprocessed.writerow(row)\n",
    "        # do not print if any other line type\n",
    "        else:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A,1287,1,International AutoRoute,/autoroute\r",
      "\r\n",
      "A,1288,1,library,/library\r",
      "\r\n",
      "A,1289,1,Master Chef Product Information,/masterchef\r",
      "\r\n",
      "A,1297,1,Central America,/centroam\r",
      "\r\n",
      "A,1215,1,For Developers Only Info,/developer\r",
      "\r\n",
      "A,1279,1,Multimedia Golf,/msgolf\r",
      "\r\n",
      "A,1239,1,Microsoft Consulting,/msconsult\r",
      "\r\n",
      "A,1282,1,home,/home\r",
      "\r\n",
      "A,1251,1,Reference Support,/referencesupport\r",
      "\r\n",
      "A,1121,1,Microsoft Magazine,/magazine\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head -10 anonymous-msweb-attributes.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA: Find the most frequent pages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MostFrequentVisits.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MostFrequentVisits.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob \n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import RawProtocol\n",
    "\n",
    "class MRSortedVisits(MRJob):\n",
    "    \n",
    "    # ----------------------------- options -------------------------- #\n",
    "    # change protocols for sorting\n",
    "#    MRJob.SORT_VALUES = True  \n",
    "    OUTPUT_PROTOCOL = RawProtocol\n",
    "    INTERNAL_PROTOCOL = RawProtocol\n",
    "\n",
    "    # -------------------- step definitions ------------------------- #\n",
    "    \n",
    "    def mapper_visit_emitter(self,_,line):\n",
    "        '''Emits page id and a count of 1 for each line in the data set'''\n",
    "        tokens= line.split(',')\n",
    "        yield tokens[1],'1'\n",
    "        \n",
    "    \n",
    "    def reducer_visit_counter(self,page,counts):\n",
    "        '''Sums up page visits for each page id'''\n",
    "        #the internal RawProtocol requires to loop over the values (counts) for a given key (page)\n",
    "        count_page = 0\n",
    "        for count in counts:\n",
    "            count_page += int(count)\n",
    "        #RawProtocol expects strings\n",
    "        yield str(count_page),str(page)\n",
    "    \n",
    "    def reducer_top_5_counter(self):\n",
    "        '''Keeps track of reducer calls which coincides with highest freq. page visits \n",
    "           if the input array is reverse sorted'''\n",
    "        self.top_n = 0    \n",
    "    \n",
    "    def identity_reducer(self,count,page):\n",
    "        '''Emits the first 5 records'''\n",
    "        self.top_n += 1\n",
    "        # page is read as a generator, exhaust it\n",
    "        if self.top_n <= 5:\n",
    "            for p in page:\n",
    "                yield str(count),p\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    def steps(self): return [\n",
    "        # Step 1: emit visit frequencies\n",
    "        MRStep(mapper=self.mapper_visit_emitter,\n",
    "               reducer=self.reducer_visit_counter,\n",
    "              ),\n",
    "        # Step 2: sort in reverse order by visit frequency    \n",
    "        MRStep(reducer_init = self.reducer_top_5_counter,\n",
    "                reducer=self.identity_reducer,\n",
    "               jobconf={\n",
    "                      'stream.num.map.output.key.fields':'2',\n",
    "                      'stream.map.output.field.separator':'\\t', \n",
    "                      'mapreduce.job.output.key.comparator.class':'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "                      'mapreduce.partition.keycomparator.options': '-k1,1nr'})\n",
    "        ]\n",
    "    \n",
    "# ---------------------------------------------- run ------------------------------------------ #\n",
    "    \n",
    "if __name__ == '__main__': \n",
    "    MRSortedVisits.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x MostFrequentVisits.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count    PageID\n",
      "---------------\n",
      "10816\t1008\t\n",
      "9370\t1034\t\n",
      "8451\t1004\t\n",
      "5325\t1018\t\n",
      "5104\t1017\t\n"
     ]
    }
   ],
   "source": [
    "!echo 'Count    PageID'\n",
    "!echo '---------------'\n",
    "!python MostFrequentVisits.py anonymous-msweb-preprocessed.data -r hadoop -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the most frequent visitor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mostFrequentVisitors.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mostFrequentVisitors.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "\n",
    "\n",
    "from mrjob.job import MRJob \n",
    "from mrjob.step import MRStep\n",
    "#from mrjob.protocol import RawProtocol\n",
    "from collections import defaultdict\n",
    "\n",
    "class MRmostFrequentVisitors(MRJob):\n",
    "    \n",
    "\n",
    "    # -------------------- step definitions ------------------------- #\n",
    "    def mapper_visitor_emitter(self,_,line):\n",
    "        tokens= line.split(',')\n",
    "        yield tokens[1],[tokens[-1],1]\n",
    "        \n",
    "    def reducer_visitor_dictionary(self):\n",
    "        # save {page:{visitor:count}} in a nested dictionary with a 0 default count\n",
    "        self.visitors = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    def reducer_visitor_counter(self,page,visitor_count):\n",
    "        for visitor,count in visitor_count:\n",
    "#            page_visitor_key = page+':'+visitor\n",
    "            self.visitors[page][visitor] += int(count)\n",
    "        yield page,self.visitors\n",
    "    \n",
    "\n",
    "    def reducer_max_freq(self,page,visitor_counts):\n",
    "        for visitor_count in visitor_counts:\n",
    "            max_key = max(visitor_count[page], key=lambda key: visitor_count[page][key])\n",
    "            yield page,max_key\n",
    "        \n",
    "    def steps(self): return [\n",
    "        # Step 1: emit visit frequencies\n",
    "        MRStep(mapper=self.mapper_visitor_emitter,\n",
    "               reducer_init=self.reducer_visitor_dictionary,\n",
    "               reducer=self.reducer_visitor_counter#,\n",
    "              ),\n",
    "        # Step 2: emit highest frequency visitor (emit only one in case of tie)\n",
    "        MRStep(reducer=self.reducer_max_freq)\n",
    "        ]\n",
    "    \n",
    "# ---------------------------------------------- run ------------------------------------------ #\n",
    "    \n",
    "if __name__ == '__main__': \n",
    "    MRmostFrequentVisitors.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mostFrequentVisitors.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/mostFrequentVisitors.root.20170205.131429.434179\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "Streaming final output from /tmp/mostFrequentVisitors.root.20170205.131429.434179/output...\n",
      "Removing temp directory /tmp/mostFrequentVisitors.root.20170205.131429.434179...\n"
     ]
    }
   ],
   "source": [
    "!python mostFrequentVisitors.py anonymous-msweb-preprocessed.data -r local > mostFrequentVisitors.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge and print the results with the attributes file which contains the urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_id\tvisitor_id\turl\n",
      "------------------------------\n",
      "1000\t36585\t/regwiz\r\n",
      "\n",
      "1001\t23995\t/support\r\n",
      "\n",
      "1002\t35235\t/athome\r\n",
      "\n",
      "1003\t35546\t/kb\r\n",
      "\n",
      "1004\t35540\t/search\r\n",
      "\n",
      "1005\t10004\t/norge\r\n",
      "\n",
      "1006\t27495\t/misc\r\n",
      "\n",
      "1007\t19492\t/ie_intl\r\n",
      "\n",
      "1008\t35236\t/msdownload\r\n",
      "\n",
      "1009\t23995\t/windows\r\n",
      "\n",
      "1010\t20915\t/vbasic\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# initialize a dictionary for easy merging based on keys\n",
    "attributes = {}\n",
    "# open both files that should be merged\n",
    "\n",
    "# ------------------------------- read in data -------------------------------------------- #\n",
    "with open('anonymous-msweb-attributes.data','r') as attribute_file, open('mostFrequentVisitors.tsv','r') as top_freq_file:\n",
    "    # read in urls and save them in a dictionary with page_id as an index\n",
    "    for row in attribute_file:\n",
    "        row = row.split(',')\n",
    "        page_id,url = row[1],row[-1]\n",
    "        attributes[page_id] = url\n",
    "    # read in page_id and visitor_id\n",
    "    for n,row in enumerate(top_freq_file):\n",
    "        page_id,visitor_id = re.findall(r'[0-9]+',row)\n",
    "        \n",
    "# ------------------------------- print first 10 lines --------------------------------- #\n",
    "        if n == 0:\n",
    "            print 'page_id\\tvisitor_id\\turl'\n",
    "            print '-'*30\n",
    "        print '{0}\\t{1}\\t{2}'.format(page_id,visitor_id,attributes[page_id])\n",
    "        if n == 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Clustering Tweet Dataset\n",
    "\n",
    "Clustering a dataset consisting of word-frequency distributions \n",
    "for 1,000 Twitter users. These Twitter users use language in very different ways,\n",
    "and were classified by hand according to the criteria:\n",
    "\n",
    "0: Human, where only basic human-human communication is observed.\n",
    "\n",
    "1: Cyborg, where language is primarily borrowed from other sources\n",
    "(e.g., jobs listings, classifieds postings, advertisements, etc...).\n",
    "\n",
    "2: Robot, where language is formulaically derived from unrelated sources\n",
    "(e.g., weather/seismology, police/fire event logs, etc...).\n",
    "\n",
    "3: Spammer, where language is replicated to high multiplicity\n",
    "(e.g., celebrity obsessions, personal promotion, etc... )\n",
    "\n",
    "Check out the preprints of  recent research,\n",
    "which spawned this dataset:\n",
    "\n",
    "* http://arxiv.org/abs/1505.04342\n",
    "* http://arxiv.org/abs/1508.01843\n",
    "\n",
    "The main data lie in the accompanying file:\n",
    "\n",
    "* [topUsers_Apr-Jul_2014_1000-words.txt](https://www.dropbox.com/s/6129k2urvbvobkr/topUsers_Apr-Jul_2014_1000-words.txt?dl=0)\n",
    "\n",
    "and are of the form:\n",
    "\n",
    "USERID,CODE,TOTAL,WORD1_COUNT,WORD2_COUNT,...\n",
    ".\n",
    ".\n",
    "\n",
    "where\n",
    "\n",
    "USERID = unique user identifier\n",
    "CODE = 0/1/2/3 class code\n",
    "TOTAL = sum of the word counts\n",
    "\n",
    "Using this data, you will implement a 1000-dimensional K-means algorithm in MrJob on the users\n",
    "by their 1000-dimensional word stripes/vectors using several \n",
    "centroid initializations and values of K.\n",
    "\n",
    "Note that each \"point\" is a user as represented by 1000 words, and that\n",
    "word-frequency distributions are generally heavy-tailed power-laws\n",
    "(often called Zipf distributions), and are very rare in the larger class\n",
    "of discrete, random distributions. For each user you will have to normalize\n",
    "by its \"TOTAL\" column. __Try several parameterizations and initializations__ :\n",
    "\n",
    "* (A) K=4 uniform random centroid-distributions over the 1000 words (generate 1000 random numbers and normalize the vectors)\n",
    "* (B) K=2 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution \n",
    "* (C) K=4 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution \n",
    "* (D) K=4 \"trained\" centroids, determined by the sums across the classes. Use use the \n",
    "(row-normalized) class-level aggregates as 'trained' starting centroids (i.e., the training is already done for you!).\n",
    "Note that you do not have to compute the aggregated distribution or the \n",
    "class-aggregated distributions, which are rows in the auxiliary file:\n",
    "\n",
    "\n",
    "* [topUsers_Apr-Jul_2014_1000-words_summaries.txt](https://www.dropbox.com/s/w4oklbsoqefou3b/topUsers_Apr-Jul_2014_1000-words_summaries.txt?dl=0)\n",
    "\n",
    "Row 1: Words\n",
    "Row 2: Aggregated distribution across all classes\n",
    "Row 3-6 class-aggregated distributions for clases 0-3\n",
    "For (A),  we select 4 users randomly from a uniform distribution [1,...,1,000]\n",
    "For (B), (C), and (D)  you will have to use data from the auxiliary file: \n",
    "\n",
    "* [topUsers_Apr-Jul_2014_1000-words_summaries.txt](https://www.dropbox.com/s/w4oklbsoqefou3b/topUsers_Apr-Jul_2014_1000-words_summaries.txt?dl=0)\n",
    "\n",
    "This file contains 5 special word-frequency distributions:\n",
    "\n",
    "* (1) The 1000-user-wide aggregate, which you will perturb for initializations\n",
    "in parts (B) and (C), and\n",
    "* (2-5) The 4 class-level aggregates for each of the user-type classes (0/1/2/3)\n",
    "\n",
    "\n",
    "In parts (B) and (C), you will have to perturb the 1000-user aggregate \n",
    "(after initially normalizing by its sum, which is also provided).\n",
    "So if in (B) you want to create 2 perturbations of the aggregate, start\n",
    "with (1), normalize, and generate 1000 random numbers uniformly \n",
    "from the unit interval (0,1) twice (for two centroids), using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "numbers = random.sample(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take these 1000 numbers and add them (component-wise) to the 1000-user aggregate,\n",
    "and then renormalize to obtain one of your aggregate-perturbed initial centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################################################################################\n",
    "##Geneate random initial centroids around the global aggregate\n",
    "##Part (B) and (C) of this question\n",
    "###################################################################################\n",
    "def startCentroidsBC(k):\n",
    "    counter = 0\n",
    "    for line in open(\"topUsers_Apr-Jul_2014_1000-words_summaries.txt\").readlines():\n",
    "        if counter == 2:        \n",
    "            data = re.split(\",\",line)\n",
    "            globalAggregate = [float(data[i+3])/float(data[2]) for i in range(1000)]\n",
    "        counter += 1\n",
    "    #perturb the global aggregate for the four initializations    \n",
    "    centroids = []\n",
    "    for i in range(k):\n",
    "        rndpoints = random.sample(1000)\n",
    "        peturpoints = [rndpoints[n]/10+globalAggregate[n] for n in range(1000)]\n",
    "        centroids.append(peturpoints)\n",
    "        total = 0\n",
    "        for j in range(len(centroids[i])):\n",
    "            total += centroids[i][j]\n",
    "        for j in range(len(centroids[i])):\n",
    "            centroids[i][j] = centroids[i][j]/total\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For experiments A, B, C and D and iterate until a threshold (try 0.001) is reached.\n",
    "After convergence, print out a summary of the classes present in each cluster.\n",
    "In particular, report the composition as measured by the total\n",
    "portion of each class type (0-3) contained in each cluster,\n",
    "and discuss your findings and any differences in outcomes across parts A-D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>K-Means</h2>\n",
    "K-means is a clustering method that aims to find the positions Î¼i,i=1...k of the clusters that minimize the distance from the data points to the cluster. K-means clustering solves:\n",
    "<br><br>\n",
    "$$\\arg\\min_{c} \\sum_{i=1}^k\\sum_{{x}\\in c_i} d({x},\\mu_i) = \\arg\\min_{c} \\sum_{i=1}^k\\sum_{{x}\\in c_i} \\left\\Vert {x}-\\mu_i \\right\\Vert_2^2$$\n",
    "<br><br>\n",
    "where ${c}_i$ is the set of points that belong to cluster i. The K-means clustering uses the square of the Euclidean distance $d({x},\\mu_i) = \\left\\Vert {x}-\\mu_i \\right\\Vert_2^2$. This problem is not trivial (in fact it is NP-hard), so the K-means algorithm only hopes to find the global minimum, possibly getting stuck in a different solution.\n",
    "\n",
    "<h2>K-means algorithm</h2>\n",
    "\n",
    "The Lloyd's algorithm, mostly known as k-means algorithm, is used to solve the k-means clustering problem and works as follows. First, decide the number of clusters k. Then:\n",
    "\n",
    "<table>\n",
    "<tbody><tr><td>1. Initialize the center of the clusters</td>\n",
    "<td>${\\mu}_i = $ some value $, i=1,...,k$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>2. Attribute the closest cluster to each data point</td>\n",
    "<td>${c}_i = \\{j: d({x}_j, \\mu_i) \\le d({x}_j, \\mu_l),  l \\ne i, j=1,...,n\\}$ </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>3. Set the position of each cluster to the mean of all data points belonging to that cluster</td>\n",
    "<td>$\\mu_i = \\frac{1}{|c_i|}\\sum_{j\\in c_i} {x}_j,\\forall i$</td>\n",
    "</tr>\n",
    "<tr><td>4. Repeat steps 2-3 until convergence</td>\n",
    "<td></td>\n",
    "</tr>\n",
    "<tr><td>Notation</td><td>${|c|} = $ number of elements in  ${c}$</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Calculating purity</h2>\n",
    "![purity illustration](http://www.candpgeneration.com/images/purity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data and save it in 'Kmeansdata.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('topUsers_Apr-Jul_2014_1000-words.txt', 'r') as infile, open('Kmeandata.csv', 'w') as outfile:\n",
    "    for line in infile:\n",
    "        line = line.split(',')\n",
    "        USERID,CODE,TOTAL,word_freqs = line[0],line[1],line[2],line[3:] \n",
    "        # keep classification information for counting\n",
    "        out = list(CODE)\n",
    "        # save classification information, normalize by dividing by total word counts\n",
    "        [out.append(float(word_freq)/float(TOTAL)) for word_freq in word_freqs]\n",
    "        outfile.write(','.join(str(j) for j in out)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Kmeans.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Kmeans.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import numpy as np\n",
    "from numpy import argmin, array, random\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from itertools import chain\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "def normalize(x):\n",
    "    return x/np.sum(x)\n",
    "\n",
    "def add_vectors(x,y):\n",
    "    return map(sum,zip(x,y))\n",
    "\n",
    "def purity(freq_table):\n",
    "    majority = np.max(freq_table,axis=1)\n",
    "    purity = float(np.sum(majority))/np.sum(freq_table)\n",
    "    return purity\n",
    "\n",
    "def uniform_centroid_generator(k,size):\n",
    "    '''Generate k x size dimensional uniformly distributed, normalized numbers.'''\n",
    "    \n",
    "    centroid_points = []\n",
    "    # generate normalized uniform random numbers\n",
    "    for i in range(k):\n",
    "        norm_unif_nums = normalize(np.random.uniform(size=size))\n",
    "        centroid_points.append(norm_unif_nums)\n",
    "     \n",
    "    # ----------------- output    \n",
    "    # save the centroid_points locally as a k x size dimensional file\n",
    "    with open('Centroids.txt','w+') as f:\n",
    "        f.writelines(','.join(str(j) for j in i) + '\\n' for i in centroid_points)\n",
    "\n",
    "    return centroid_points\n",
    "\n",
    "        \n",
    "def perturbation_centroid_generator(k,size):\n",
    "    counter = 0\n",
    "    centroid_points=[]\n",
    "    \n",
    "    # Read in the user-wide aggregate, then normalize\n",
    "    with open('topUsers_Apr-Jul_2014_1000-words_summaries.txt','r') as infile:\n",
    "        for line in infile:\n",
    "            line = line.split(',')\n",
    "            if counter == 1:\n",
    "                globalAggregate = [int(word_sum)/float(line[2]) for word_sum in line[3:]]\n",
    "            counter += 1\n",
    "        # Perturb the globalAggregate for each initialization\n",
    "    for i in range(k):\n",
    "        rndpoints = random.sample(size)\n",
    "        pert_centroid_points = add_vectors(globalAggregate,rndpoints/10)\n",
    "        norm_pert_centroid_points = normalize(pert_centroid_points)\n",
    "        centroid_points.append(norm_pert_centroid_points)\n",
    "    # ----------------- output    \n",
    "    with open('Centroids.txt', 'w+') as f:\n",
    "         f.writelines(','.join(str(j) for j in i) + '\\n' for i in centroid_points)\n",
    "            \n",
    "    return centroid_points\n",
    "    \n",
    "\n",
    "def trained_centroid_generator(k):\n",
    "    \n",
    "    centroid_points=[]\n",
    "    \n",
    "    # Read in the user-wide aggregate, then normalize\n",
    "    with open('topUsers_Apr-Jul_2014_1000-words_summaries.txt','r') as infile:\n",
    "        for line in infile:\n",
    "            line = line.split(',')\n",
    "            if line[0]=='CODE':\n",
    "                aggregate = [int(word_freq)/float(line[2]) for word_freq in line[3:]]\n",
    "                centroid_points.append(aggregate)\n",
    "  \n",
    "    with open('Centroids.txt', 'w+') as f:\n",
    "         f.writelines(','.join(str(j) for j in i) + '\\n' for i in centroid_points)\n",
    "            \n",
    "    return centroid_points\n",
    "    \n",
    "def centroid_generator(centroid_type,k,size=1000):\n",
    "    \n",
    "    if centroid_type == 'uniform':\n",
    "        return uniform_centroid_generator(k,size)\n",
    "        \n",
    "    elif centroid_type == 'perturbation':\n",
    "        return perturbation_centroid_generator(k,size)\n",
    "        \n",
    "    elif centroid_type == 'trained':\n",
    "        return trained_centroid_generator(k)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Unknown centroid type. Choose 'uniform', 'perturbation' or 'trained'.\")\n",
    "\n",
    "#Calculate find the nearest centroid for data point \n",
    "def MinDist(datapoint, centroid_points):\n",
    "    datapoint = array(datapoint)\n",
    "    centroid_points = array(centroid_points)\n",
    "    diff = datapoint - centroid_points \n",
    "    diffsq = diff*diff\n",
    "    # Get the nearest centroid for each instance\n",
    "    minidx = argmin(list(diffsq.sum(axis = 1)))\n",
    "    return minidx\n",
    "\n",
    "#Check whether centroids converge\n",
    "def stop_criterion(centroid_points_old, centroid_points_new,T):\n",
    "    oldvalue = list(chain(*centroid_points_old))\n",
    "    newvalue = list(chain(*centroid_points_new))\n",
    "    Diff = [abs(x-y) for x, y in zip(oldvalue, newvalue)]\n",
    "    Flag = True\n",
    "    for i in Diff:\n",
    "        if(i>T):\n",
    "            Flag = False\n",
    "            break\n",
    "    return Flag\n",
    "\n",
    "class MRKmeans(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init = self.mapper_init, \n",
    "                   mapper=self.mapper,\n",
    "                   combiner_init = self.combiner_init,\n",
    "                   combiner = self.combiner,\n",
    "                   reducer_init = self.reducer_init,\n",
    "                   reducer=self.reducer)\n",
    "               ]\n",
    "    #load centroids info from file\n",
    "    def mapper_init(self):        \n",
    "        self.centroid_points = [map(float,s.split('\\n')[0].split(',')) for s in open(\"Centroids.txt\").readlines()]\n",
    "        #open('Centroids.txt', 'w').close()\n",
    "\n",
    "    #load data and output the nearest centroid index and data point \n",
    "    def mapper(self, _, line):\n",
    "        D = (map(float,line.split(',')))\n",
    "        # first element is the classification, to provide a counter, convert it to a vector of length 4\n",
    "        # with [#Human,#Cyborg,#Robot,#Spammer]\n",
    "        class_freqs = [0]*4\n",
    "        class_freqs[int(D[0])] = 1\n",
    "        # normalized frequencies start from the second element\n",
    "        norm_freqs = D[1:]\n",
    "        # Output:\n",
    "        yield int(MinDist(norm_freqs,self.centroid_points)), (norm_freqs,class_freqs,1)\n",
    "        \n",
    "    def combiner_init(self):\n",
    "        self.n = 1000\n",
    "        \n",
    "    #Combine sum of data points locally\n",
    "    def combiner(self, idx, inputdata):\n",
    "        norm_freqs_sum = [0]*self.n\n",
    "        class_freqs_sum = [0]*4\n",
    "        count = 0\n",
    "        \n",
    "        for norm_freqs, class_freqs, n in inputdata:\n",
    "            count += n\n",
    "            class_freqs_sum = add_vectors(class_freqs_sum,class_freqs)\n",
    "            norm_freqs_sum = add_vectors(norm_freqs_sum,norm_freqs)\n",
    "        \n",
    "        yield idx,(norm_freqs_sum,class_freqs_sum,count)\n",
    "        \n",
    "    def reducer_init(self):\n",
    "        self.n = 1000\n",
    "        \n",
    "    #Aggregate sum for each cluster and then calculate the new centroids\n",
    "    def reducer(self, idx, inputdata): \n",
    "        centroids = []\n",
    "        count = 0 \n",
    "        norm_freqs = [0]*self.n\n",
    "        class_freqs = [0]*4\n",
    "        for norm_freqs_sum, class_freqs_sum, n in inputdata:\n",
    "            count += n\n",
    "            class_freqs = add_vectors(class_freqs_sum,class_freqs)\n",
    "            norm_freqs =  add_vectors(norm_freqs_sum,norm_freqs)\n",
    "        \n",
    "        # the new centroids are the means of the sums of all the member vector elements\n",
    "        centroids = [x/count for x in norm_freqs_sum]\n",
    "\n",
    "        yield idx,(centroids,class_freqs)\n",
    "    \n",
    "      \n",
    "if __name__ == '__main__':\n",
    "    MRKmeans.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting kmeans_runner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile kmeans_runner.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from Kmeans import MRKmeans, stop_criterion, centroid_generator, purity\n",
    "\n",
    "#mr_job = MRKmeans(args=['Kmeandata.csv', '--file=Centroids.txt','--cmdenv, 'k=4'])\n",
    "mr_job = MRKmeans(args=['Kmeandata.csv', '--file=Centroids.txt'])\n",
    "\n",
    "\n",
    "# ------------------------------ command line arguments ------------------#\n",
    "k = int(sys.argv[1])\n",
    "centroid_type =  sys.argv[2]\n",
    "# bootstrapping is an embarissingly parallel problem but solve it now serially\n",
    "bootstrap_rep = int(sys.argv[3])\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------- initialize ------------------------------- #\n",
    "\n",
    "\n",
    "# set up containers\n",
    "\n",
    "iterations = []\n",
    "purities = []\n",
    "\n",
    "for n_boot in range(bootstrap_rep):\n",
    "    # set boostrap iteration counter\n",
    "    print('\\rBootstrap replications:',n_boot+1,'/',bootstrap_rep,end='')\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    #Geneate initial centroids\n",
    "    centroid_points = centroid_generator(centroid_type,k)\n",
    "\n",
    "    # Update centroids iteratively\n",
    "    i = 0\n",
    "    class_freqs=[[0]*4]*k\n",
    "    while(1):\n",
    "        # save previous centoids to check convergency\n",
    "        centroid_points_old = list(centroid_points)\n",
    "    \n",
    "        with mr_job.make_runner() as runner: \n",
    "            runner.run()\n",
    "            # stream_output: get access of the output \n",
    "            for line in runner.stream_output():\n",
    "                key,value =  mr_job.parse_output_line(line)\n",
    "                centroid_points[key] = value[0]\n",
    "                class_freqs[key] = value[1]   \n",
    "                    \n",
    "            # Update the centroids for the next iteration\n",
    "            with open('Centroids.txt', 'w') as f:\n",
    "                f.writelines(','.join(str(j) for j in i) + '\\n' for i in centroid_points)\n",
    "\n",
    "        i = i + 1\n",
    "        if(stop_criterion(centroid_points_old,centroid_points,0.01)):\n",
    "            break\n",
    "    # save purity and iteration counts for the bootstrap replicates        \n",
    "    purities.append(purity(class_freqs))\n",
    "    iterations.append(i)\n",
    "\n",
    "# ------------------------------------- report results -------------------------- #\n",
    "\n",
    "#  ------------------ info line\n",
    "print('\\nCluster type: {0}\\n'.format(sys.argv[2]))\n",
    "print('Number of clusters: {0}\\n'.format(sys.argv[1]))\n",
    "\n",
    "\n",
    "\n",
    "# ------------------ table \n",
    "\n",
    "print('\\nProportions of classes in clusters are based on the last bootstrap replicate.\\n')\n",
    "\n",
    "# proportions of classes and clusters\n",
    "obs_class = [0]*4\n",
    "with open('topUsers_Apr-Jul_2014_1000-words.txt') as infile:\n",
    "    for line in infile:\n",
    "        line = line.split(',')\n",
    "        obs_class[int(line[1])] += 1\n",
    "\n",
    "class_template = '{0:>7} | {1:5} | {2:6} | {3:5} | {4:7} | {5:^4}'       \n",
    "prop_template = '{0:7} | {1:5.2f} | {2:6.2f} | {3:5.2f} | {4:7.2f} | {5:4}'     \n",
    "\n",
    "# header\n",
    "print(class_template.format('Cluster','Human','Cyborg','Robot','Spammer','N'))\n",
    "print('-'*49)\n",
    "# values\n",
    "for j in range(len(class_freqs)):\n",
    "    # cluster membership distribution\n",
    "    col_sums = np.sum(class_freqs,axis=1)\n",
    "    # class predictions\n",
    "    pred_class = list(class_freqs[j])\n",
    "    props = [float(x)/y for x,y  in zip(pred_class,obs_class)]\n",
    "    print(prop_template.format(j,props[0],props[1],props[2],props[3],col_sums[j]))\n",
    "# footer\n",
    "print('-'*49)\n",
    "row_sums = np.sum(class_freqs,axis=0)\n",
    "total = np.sum(class_freqs)\n",
    "print(class_template.format('N',row_sums[0],row_sums[1],row_sums[2],row_sums[3],total))\n",
    "\n",
    "# ----------------------- Statistics\n",
    "\n",
    "# purity\n",
    "\n",
    "mean_purity = np.mean(purities)\n",
    "purity_q05 = np.round(np.percentile(purities,5),2)\n",
    "purity_q95 = np.round(np.percentile(purities,95),2)\n",
    "print('\\nMean purity (95% conf. int.): {0} ({1}-{2})'.format(mean_purity,purity_q05,purity_q95))\n",
    "# iterations\n",
    "median_iteration = int(np.median(iterations))\n",
    "iteration_q05 = int(np.ceil(np.percentile(iterations,5)))\n",
    "iteration_q95 = int(np.ceil(np.percentile(iterations,95)))\n",
    "print('\\nMedian number of iterations (95% conf. int.): {0} ({1}-{2})\\n'.format(median_iteration,iteration_q05,iteration_q95))\n",
    "      \n",
    "    \n",
    "        \n",
    "#print centroid_points\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A: Uniform, 4 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap replications: 100 / 100\n",
      "Cluster type: uniform\n",
      "\n",
      "Number of clusters: 4\n",
      "\n",
      "\n",
      "Proportions of classes in clusters are based on the last bootstrap replicate.\n",
      "\n",
      "Cluster | Human | Cyborg | Robot | Spammer |  N  \n",
      "-------------------------------------------------\n",
      "      0 |  0.01 |   0.00 |  0.22 |    0.67 |   88\n",
      "      1 |  0.00 |   0.59 |  0.06 |    0.00 |   57\n",
      "      2 |  0.98 |   0.02 |  0.04 |    0.25 |  769\n",
      "      3 |  0.01 |   0.38 |  0.69 |    0.08 |   86\n",
      "-------------------------------------------------\n",
      "      N |   752 |     91 |    54 |     103 | 1000\n",
      "\n",
      "Mean purity (95% conf. int.): 0.86495 (0.83-0.91)\n",
      "\n",
      "Median number of iterations (95% conf. int.): 4 (3-7)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python kmeans_runner.py 4 uniform 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B: perturbation, 2 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap replications: 100 / 100\n",
      "Cluster type: perturbation\n",
      "\n",
      "Number of clusters: 2\n",
      "\n",
      "\n",
      "Proportions of classes in clusters are based on the last bootstrap replicate.\n",
      "\n",
      "Cluster | Human | Cyborg | Robot | Spammer |  N  \n",
      "-------------------------------------------------\n",
      "      0 |  1.00 |   0.03 |  0.09 |    0.89 |  850\n",
      "      1 |  0.00 |   0.97 |  0.91 |    0.11 |  150\n",
      "-------------------------------------------------\n",
      "      N |   752 |     91 |    54 |     103 | 1000\n",
      "\n",
      "Mean purity (95% conf. int.): 0.83414 (0.82-0.84)\n",
      "\n",
      "Median number of iterations (95% conf. int.): 3 (2-4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python kmeans_runner.py 2 perturbation 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C: perturbation, 4 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap replications: 100 / 100\n",
      "Cluster type: perturbation\n",
      "\n",
      "Number of clusters: 4\n",
      "\n",
      "\n",
      "Proportions of classes in clusters are based on the last bootstrap replicate.\n",
      "\n",
      "Cluster | Human | Cyborg | Robot | Spammer |  N  \n",
      "-------------------------------------------------\n",
      "      0 |  0.00 |   0.63 |  0.06 |    0.00 |   60\n",
      "      1 |  0.03 |   0.36 |  0.67 |    0.22 |  111\n",
      "      2 |  0.97 |   0.01 |  0.15 |    0.78 |  822\n",
      "      3 |  0.00 |   0.00 |  0.13 |    0.00 |    7\n",
      "-------------------------------------------------\n",
      "      N |   752 |     91 |    54 |     103 | 1000\n",
      "\n",
      "Mean purity (95% conf. int.): 0.86188 (0.83-0.91)\n",
      "\n",
      "Median number of iterations (95% conf. int.): 4 (3-7)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python kmeans_runner.py 4 perturbation 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D: trained \n",
    "\n",
    "As randomness is not involved in the centroid generation, bootstrapping is not necessary to estimate the purity scores and iteration numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap replications: 1 / 1\n",
      "Cluster type: trained\n",
      "\n",
      "Number of clusters: 4\n",
      "\n",
      "\n",
      "Proportions of classes in clusters are based on the last bootstrap replicate.\n",
      "\n",
      "Cluster | Human | Cyborg | Robot | Spammer |  N  \n",
      "-------------------------------------------------\n",
      "      0 |  0.98 |   0.02 |  0.06 |    0.19 |  763\n",
      "      1 |  0.00 |   0.63 |  0.06 |    0.00 |   60\n",
      "      2 |  0.00 |   0.35 |  0.89 |    0.07 |   90\n",
      "      3 |  0.01 |   0.00 |  0.00 |    0.74 |   87\n",
      "-------------------------------------------------\n",
      "      N |   752 |     91 |    54 |     103 | 1000\n",
      "\n",
      "Mean purity (95% conf. int.): 0.919 (0.92-0.92)\n",
      "\n",
      "Median number of iterations (95% conf. int.): 2 (2-2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python kmeans_runner.py 4 trained 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Discussion\n",
    "\n",
    "Choosing centroids from a uniform distribution implies in a Bayesian sense that we do not have any prior information on the final centroids. Therefore, it is surprizing that that the perturbed global aggregate centroids return the same purity and same number of iterations as the uniformly chosen one. However, we have not tested the accuracy, recall or other measures of classification, but we can surmise that the perturbed global aggregates might outperform the uniform one according to these measures. \n",
    "\n",
    "It is also possible to observe that identifying more clusters require more iterations in general. However, if the cluster centroids are already trained as in the trained centroid example, the convergence is fast, and the stopping criterion is reached quickly in 2 iteratiions with 4 clusters. The purity of the trained clustering algorithm is the highest as well, 10% higher than the one of the 2 cluster perturbed and about 5% higher than the ones of the 4 cluster perturbed and uniformly chosen centroids."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python2]",
   "language": "python",
   "name": "conda-env-python2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
