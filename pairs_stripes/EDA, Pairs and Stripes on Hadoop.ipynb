{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA, Pairs and Stripes on Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-01-27 04:28:11--  https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv?dl=0\n",
      "Resolving www.dropbox.com... 162.125.65.1\n",
      "Connecting to www.dropbox.com|162.125.65.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://dl.dropboxusercontent.com/content_link/lTCiJCchrbPy2xfvRCGbPkOLVweAEPNXtByrOgoK90VPHNJbzFX1fD0DvVjf2l5w/file [following]\n",
      "--2017-01-27 04:28:12--  https://dl.dropboxusercontent.com/content_link/lTCiJCchrbPy2xfvRCGbPkOLVweAEPNXtByrOgoK90VPHNJbzFX1fD0DvVjf2l5w/file\n",
      "Resolving dl.dropboxusercontent.com... 162.125.65.6\n",
      "Connecting to dl.dropboxusercontent.com|162.125.65.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 50906486 (49M) [text/csv]\n",
      "Saving to: “Consumer_Complaints.csv”\n",
      "\n",
      "100%[======================================>] 50,906,486  7.92M/s   in 6.5s    \n",
      "\n",
      "2017-01-27 04:28:19 (7.43 MB/s) - “Consumer_Complaints.csv” saved [50906486/50906486]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download data\n",
    "!wget https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv?dl=0 -O Consumer_Complaints.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /home/cloudera/dfs/Consumer_Complaints/Consumer_Complaints.csv\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm  /home/cloudera/dfs/Consumer_Complaints/Consumer_Complaints.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create HDFS directories\n",
    "!hdfs dfs -mkdir -p /home/cloudera/dfs/Consumer_Complaints\n",
    "!hdfs dfs -mkdir -p /home/cloudera/dfs/short-text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/home/cloudera/dfs/short-text/input.txt': File exists\r\n"
     ]
    }
   ],
   "source": [
    "# Put the data into HDFS\n",
    "!hdfs dfs -put Consumer_Complaints.csv /home/cloudera/dfs/Consumer_Complaints\n",
    "!hdfs dfs -put input.txt /home/cloudera/dfs/short-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting complaintCountsMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile complaintCountsMapper.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "\n",
    "import sys \n",
    "from csv import reader\n",
    "\n",
    "# ------------------------ define function ------------------------------ #\n",
    "\n",
    "def mapper_runner(csv_file):\n",
    "    for row in reader(csv_file):\n",
    "\n",
    "        try:\n",
    "            # the values in the first (id) column are integers except the header\n",
    "            is_header = int(row[0])\n",
    "            # Product is the second column\n",
    "            print row[1]\n",
    "        except:\n",
    "            # if the id is not an integer, it is a header, do not map it\n",
    "            pass\n",
    "\n",
    "# -------------------------------run------------------------------------- #\n",
    "    \n",
    "if __name__== \"__main__\":\n",
    "    mapper_runner(sys.stdin)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting complaintCountsReducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile complaintCountsReducer.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "# ---------------------------- define functions-------------------------- #\n",
    "def product_counter(product_type):\n",
    "    '''Emits a Product counter of product types for the jobtracker UI'''\n",
    "    return sys.stderr.write(\"reporter:counter:Product,{0},1\\n\".format(product_type))\n",
    "\n",
    "\n",
    "def reducer_runner():\n",
    "     for product in sys.stdin:\n",
    "            product = product.strip()\n",
    "\n",
    "            # classify type of product\n",
    "            if product != 'Debt collection' and product != 'Mortgage':\n",
    "                product = 'Other'\n",
    "        # place counter in the jobtracker UI\n",
    "            sys.stderr.write(\"reporter:counter:Product,{0},1\\n\".format(product))\n",
    "\n",
    "\n",
    "# --------------------------------- run --------------------------------- #        \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    reducer_runner()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x complaintCountsReducer.py\n",
    "!chmod a+x complaintCountsMapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /home/cloudera/W261/Spring2017/HW3/hw3.1-output\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.1.jar] /tmp/streamjob6538853055586384922.jar tmpDir=null\n",
      "17/01/29 01:54:35 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/29 01:54:35 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/29 01:54:36 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/01/29 01:54:36 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/01/29 01:54:36 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1484923209747_0225\n",
      "17/01/29 01:54:37 INFO impl.YarnClientImpl: Submitted application application_1484923209747_0225\n",
      "17/01/29 01:54:37 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1484923209747_0225/\n",
      "17/01/29 01:54:37 INFO mapreduce.Job: Running job: job_1484923209747_0225\n",
      "17/01/29 01:54:44 INFO mapreduce.Job: Job job_1484923209747_0225 running in uber mode : false\n",
      "17/01/29 01:54:44 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/01/29 01:54:53 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "17/01/29 01:54:54 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/01/29 01:55:00 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/01/29 01:55:00 INFO mapreduce.Job: Job job_1484923209747_0225 completed successfully\n",
      "17/01/29 01:55:00 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5191230\n",
      "\t\tFILE: Number of bytes written=10747109\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910872\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=14030\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4932\n",
      "\t\tTotal time spent by all map tasks (ms)=14030\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4932\n",
      "\t\tTotal vcore-seconds taken by all map tasks=14030\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4932\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=14366720\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=5050368\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=312912\n",
      "\t\tMap output bytes=4565400\n",
      "\t\tMap output materialized bytes=5191236\n",
      "\t\tInput split bytes=290\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=9\n",
      "\t\tReduce shuffle bytes=5191236\n",
      "\t\tReduce input records=312912\n",
      "\t\tReduce output records=0\n",
      "\t\tSpilled Records=625824\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=200\n",
      "\t\tCPU time spent (ms)=5750\n",
      "\t\tPhysical memory (bytes) snapshot=807526400\n",
      "\t\tVirtual memory (bytes) snapshot=4679028736\n",
      "\t\tTotal committed heap usage (bytes)=936902656\n",
      "\tProduct\n",
      "\t\tDebt collection=44372\n",
      "\t\tMortgage=125752\n",
      "\t\tOther=142788\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50910582\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=0\n",
      "17/01/29 01:55:00 INFO streaming.StreamJob: Output directory: /home/cloudera/W261/Spring2017/HW3/hw3.1-output\n"
     ]
    }
   ],
   "source": [
    "# Hadoop command\n",
    "!hdfs dfs -rm -r /home/cloudera/W261/Spring2017/HW3/hw3.1-output\n",
    "\n",
    "!sudo hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.1.jar \\\n",
    "    -mapper /home/cloudera/W261/Spring2017/HW3/complaintCountsMapper.py \\\n",
    "    -reducer /home/cloudera/W261/Spring2017/HW3/complaintCountsReducer.py \\\n",
    "    -input /home/cloudera/dfs/Consumer_Complaints/Consumer_Complaints.csv  \\\n",
    "    -output /home/cloudera/W261/Spring2017/HW3/hw3.1-output \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![Product counters](counter_31.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__3.2.A__     \n",
    "Perform a word count analysis of this single record dataset using a Mapper and Reducer based WordCount (i.e., no combiners are used here) using user defined Counters to count up how many times the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing this word count job. The answer  should be 1 and 4 respectively. Please explain.\n",
    "\n",
    "__3.2.B__   \n",
    "Please use mulitple mappers and reducers for these jobs (at least 2 mappers and 2 reducers).\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper and Reducer based WordCount (i.e., no combiners used anywhere)  using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. \n",
    "\n",
    "__3.2.C__     \n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper, Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined Counters to count up how many time the mapper, combiner, reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. \n",
    "\n",
    "Using a single reducer: \n",
    "- What are the top 50 most frequent terms in your word count analysis?    \n",
    "- Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order.    \n",
    "- Present bottom 10 tokens (least frequent items). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile reducer3.2.A.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "# ---------------------- define function ---------------------------- #\n",
    "\n",
    "def reducer_runner():\n",
    "    # initialize variables\n",
    "    cur_key = None\n",
    "    cur_count = 0\n",
    "\n",
    "    # loop over STDIN\n",
    "    for line in sys.stdin:\n",
    "        # and sum the count of the same consecutive\n",
    "        key,count = line.lower().strip().split()\n",
    "    if key == cur_key:\n",
    "        cur_count += int(count)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '{0}\\t{1}'.format(cur_key,cur_count)\n",
    "    \n",
    "        cur_key = key\n",
    "        cur_count = int(count)\n",
    "\n",
    "        \n",
    "# emit the last key\n",
    "print '{0}\\t{1}'.format(cur_key,cur_count)\n",
    "\n",
    "# ------------------------------ run -------------------------------- #\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.stderr.write(\"reporter:counter:Wordcount job,Reducer,1\\n\")\n",
    "    reducer_runner()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper3.2.A.py\n",
    "!chmod a+x reducer3.2.A.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /home/cloudera/W261/Spring2017/HW3/short-text-output\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.1.jar] /tmp/streamjob84348425894680986.jar tmpDir=null\n",
      "17/01/29 01:56:56 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/29 01:56:56 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/29 01:56:57 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/01/29 01:56:57 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "17/01/29 01:56:57 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1484923209747_0226\n",
      "17/01/29 01:56:57 INFO impl.YarnClientImpl: Submitted application application_1484923209747_0226\n",
      "17/01/29 01:56:58 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1484923209747_0226/\n",
      "17/01/29 01:56:58 INFO mapreduce.Job: Running job: job_1484923209747_0226\n",
      "17/01/29 01:57:05 INFO mapreduce.Job: Job job_1484923209747_0226 running in uber mode : false\n",
      "17/01/29 01:57:05 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/01/29 01:57:11 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/01/29 01:57:21 INFO mapreduce.Job:  map 100% reduce 25%\n",
      "17/01/29 01:57:24 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "17/01/29 01:57:26 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/01/29 01:57:26 INFO mapreduce.Job: Job job_1484923209747_0226 completed successfully\n",
      "17/01/29 01:57:26 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=83\n",
      "\t\tFILE: Number of bytes written=607643\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=153\n",
      "\t\tHDFS: Number of bytes written=26\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4054\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=37000\n",
      "\t\tTotal time spent by all map tasks (ms)=4054\n",
      "\t\tTotal time spent by all reduce tasks (ms)=37000\n",
      "\t\tTotal vcore-seconds taken by all map tasks=4054\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=37000\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=4151296\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=37888000\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1\n",
      "\t\tMap output records=7\n",
      "\t\tMap output bytes=45\n",
      "\t\tMap output materialized bytes=83\n",
      "\t\tInput split bytes=122\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=83\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=4\n",
      "\t\tSpilled Records=14\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=499\n",
      "\t\tCPU time spent (ms)=4780\n",
      "\t\tPhysical memory (bytes) snapshot=1176260608\n",
      "\t\tVirtual memory (bytes) snapshot=7846580224\n",
      "\t\tTotal committed heap usage (bytes)=1049624576\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tWordcount job\n",
      "\t\tMapper=1\n",
      "\t\tReducer=4\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=31\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=26\n",
      "17/01/29 01:57:26 INFO streaming.StreamJob: Output directory: /home/cloudera/W261/Spring2017/HW3/short-text-output\n"
     ]
    }
   ],
   "source": [
    "# Hadoop command\n",
    "\n",
    "!hdfs dfs -rm -r /home/cloudera/W261/Spring2017/HW3/short-text-output\n",
    "\n",
    "!sudo hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.1.jar -D mapreduce.job.maps=1 \\\n",
    "    -D mapreduce.job.reduces=4 \\\n",
    "    -mapper /home/cloudera/W261/Spring2017/HW3/mapper3.2.A.py \\\n",
    "    -reducer /home/cloudera/W261/Spring2017/HW3/reducer3.2.A.py \\\n",
    "    -input /home/cloudera/dfs/short-text/input.txt  \\\n",
    "    -output /home/cloudera/W261/Spring2017/HW3/short-text-output \\\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### SCREENSHOT OF JOB TRACKER UI COUNTERS\n",
    "\n",
    "![](counter_32A.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.B SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper3.2.B.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper3.2.B.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys \n",
    "import re\n",
    "from csv import reader\n",
    "\n",
    "# ------------------------------ define function ---------------------------- #\n",
    "def mapper_runner():\n",
    "\n",
    "    WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "    for row in reader(sys.stdin):\n",
    "        try:\n",
    "             # the values in the first (id) column are integers except the header\n",
    "            is_header = int(row[0])\n",
    "            # Issue is the fourth column\n",
    "            issue = row[3]\n",
    "            for word in WORD_RE.findall(issue):\n",
    "                print '{0}\\t1'.format(word)\n",
    "        except:\n",
    "            # if the id can't be cast to int, it is a header, do not map it\n",
    "            pass\n",
    "\n",
    "# -------------------------------- run ---------------------------------------- # \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.stderr.write(\"reporter:counter:Wordcount,Mapper,1\\n\")\n",
    "    mapper_runner()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer3.2.B.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer3.2.B.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "# ------------------------------ define functions -------------------------------- #\n",
    "\n",
    "def reducer_runner():\n",
    "    \n",
    "    # define variables\n",
    "    cur_key = None\n",
    "    cur_count = 0\n",
    "\n",
    "    for line in sys.stdin:\n",
    "        key,count = line.lower().strip().split()\n",
    "        if key == cur_key:\n",
    "            cur_count += int(count)\n",
    "        else:\n",
    "            if cur_key:\n",
    "                print '{0}\\t{1}'.format(cur_key,cur_count)\n",
    "            cur_key = key\n",
    "            cur_count = int(count)\n",
    "\n",
    "    # emit the last key\n",
    "    print '{0}\\t{1}'.format(cur_key,cur_count)\n",
    "\n",
    "\n",
    "# ------------------------------ run -------------------------------- #\n",
    "if __name__ == \"__main__\":\n",
    "    sys.stderr.write(\"reporter:counter:Wordcount,Reducer,1\\n\")\n",
    "    reducer_runner()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper3.2.B.py\n",
    "!chmod a+x reducer3.2.B.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /home/cloudera/W261/Spring2017/HW3/hw3.2.B-output\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.1.jar] /tmp/streamjob1994643580797449871.jar tmpDir=null\n",
      "17/01/29 02:14:40 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/29 02:14:40 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/29 02:14:41 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/01/29 02:14:41 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/01/29 02:14:41 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1484923209747_0228\n",
      "17/01/29 02:14:41 INFO impl.YarnClientImpl: Submitted application application_1484923209747_0228\n",
      "17/01/29 02:14:41 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1484923209747_0228/\n",
      "17/01/29 02:14:41 INFO mapreduce.Job: Running job: job_1484923209747_0228\n",
      "17/01/29 02:14:49 INFO mapreduce.Job: Job job_1484923209747_0228 running in uber mode : false\n",
      "17/01/29 02:14:49 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/01/29 02:15:00 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/01/29 02:15:09 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "17/01/29 02:15:10 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/01/29 02:15:10 INFO mapreduce.Job: Job job_1484923209747_0228 completed successfully\n",
      "17/01/29 02:15:10 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=16121335\n",
      "\t\tFILE: Number of bytes written=32728800\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910872\n",
      "\t\tHDFS: Number of bytes written=2334\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=17064\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=14645\n",
      "\t\tTotal time spent by all map tasks (ms)=17064\n",
      "\t\tTotal time spent by all reduce tasks (ms)=14645\n",
      "\t\tTotal vcore-seconds taken by all map tasks=17064\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=14645\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=17473536\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=14996480\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=1348308\n",
      "\t\tMap output bytes=13424707\n",
      "\t\tMap output materialized bytes=16121347\n",
      "\t\tInput split bytes=290\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=187\n",
      "\t\tReduce shuffle bytes=16121347\n",
      "\t\tReduce input records=1348308\n",
      "\t\tReduce output records=187\n",
      "\t\tSpilled Records=2696616\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=761\n",
      "\t\tCPU time spent (ms)=9670\n",
      "\t\tPhysical memory (bytes) snapshot=1412059136\n",
      "\t\tVirtual memory (bytes) snapshot=6247653376\n",
      "\t\tTotal committed heap usage (bytes)=1301807104\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tWordcount\n",
      "\t\tMapper=2\n",
      "\t\tReducer=2\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50910582\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2334\n",
      "17/01/29 02:15:10 INFO streaming.StreamJob: Output directory: /home/cloudera/W261/Spring2017/HW3/hw3.2.B-output\n"
     ]
    }
   ],
   "source": [
    "# Hadoop command\n",
    "\n",
    "!hdfs dfs -rm -r /home/cloudera/W261/Spring2017/HW3/hw3.2.B-output\n",
    "\n",
    "!sudo hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.1.jar \\\n",
    "    -mapper /home/cloudera/W261/Spring2017/HW3/mapper3.2.B.py \\\n",
    "    -reducer /home/cloudera/W261/Spring2017/HW3/reducer3.2.B.py \\\n",
    "    -input /home/cloudera/dfs/Consumer_Complaints/Consumer_Complaints.csv  \\\n",
    "    -output /home/cloudera/W261/Spring2017/HW3/hw3.2.B-output \\\n",
    "    -numReduceTasks 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.2.B OUTPUT/ANSWER\n",
    "\n",
    "The default split is two mappers and I suggested Hadoop to run two reducers as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- part 0 --------------\n",
      "apr\t3431\n",
      "account\t16555\n",
      "applied\t139\n",
      "arbitration\t168\n",
      "bankruptcy\t222\n",
      "----------------- part 1-------------- \n",
      "atm\t2422\n",
      "advertising\t1193\n",
      "application\t8868\n",
      "balance\t597\n",
      "cancelling\t2795\n"
     ]
    }
   ],
   "source": [
    "!echo \"----------------- part 0 --------------\"\n",
    "!hdfs dfs -cat /home/cloudera/W261/Spring2017/HW3/hw3.2.B-output/part-00000 | head  -5\n",
    "!echo \"----------------- part 1-------------- \"\n",
    "!hdfs dfs -cat /home/cloudera/W261/Spring2017/HW3/hw3.2.B-output/part-00001 | head  -5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### INSERT SCREENSHOT OF JOB TRACKER UI COUNTERS\n",
    "\n",
    "Only the upper part of the wordcounts are shown here but, in case of interest, the counters can also be seen above in Hadoop output.\n",
    "\n",
    "![](counter-32B.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.C SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper3.2.C.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper3.2.C.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "\n",
    "import sys \n",
    "import re\n",
    "from csv import reader\n",
    "\n",
    "# ------------------------------ define function ---------------------------- #\n",
    "def mapper_runner():\n",
    "\n",
    "    WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "    for row in reader(sys.stdin):\n",
    "        try:\n",
    "             # the values in the first (id) column are integers except the header\n",
    "            is_header = int(row[0])\n",
    "            # Issue is the fourth column\n",
    "            issue = row[3]\n",
    "            for word in WORD_RE.findall(issue):\n",
    "                print '{0}\\t1'.format(word)\n",
    "        except:\n",
    "            # if the id can't be cast to int, it is a header, do not map it\n",
    "            pass\n",
    "\n",
    "# -------------------------------- run ---------------------------------------- # \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.stderr.write(\"reporter:counter:Wordcount,Mapper,1\\n\")\n",
    "    mapper_runner()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combiner3.2.C.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combiner3.2.C.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "# ------------------------------ define functions -------------------------------- #\n",
    "\n",
    "def combiner_runner():\n",
    "    \n",
    "    # define variables\n",
    "    cur_key = None\n",
    "    cur_count = 0\n",
    "    total = 0\n",
    "\n",
    "    for line in sys.stdin:\n",
    "        key,count = line.lower().strip().split()\n",
    "        # start with total counts   \n",
    "        if key == cur_key:\n",
    "            cur_count += int(count)\n",
    "        else:\n",
    "            if cur_key:\n",
    "                print '{0}\\t{1}'.format(cur_key,cur_count)\n",
    "            cur_key = key\n",
    "            cur_count = int(count)\n",
    "\n",
    "    # emit the last key\n",
    "    print '{0}\\t{1}'.format(cur_key,cur_count)\n",
    "\n",
    "\n",
    "# ------------------------------ run -------------------------------- #\n",
    "if __name__ == \"__main__\":\n",
    "    sys.stderr.write(\"reporter:counter:Wordcount,Combiner,1\\n\")\n",
    "    combiner_runner()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer3.2.C.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer3.2.C.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "# ------------------------------ define functions -------------------------------- #\n",
    "\n",
    "def reducer_runner():\n",
    "    \n",
    "    # define variables\n",
    "    cur_key = None\n",
    "    cur_count = 0\n",
    "\n",
    "    for line in sys.stdin:\n",
    "        key,count = line.lower().strip().split()\n",
    "        if key == cur_key:\n",
    "            cur_count += int(count)\n",
    "        else:\n",
    "            if cur_key:\n",
    "                print '{0}\\t{1}'.format(cur_key,cur_count)\n",
    "            cur_key = key\n",
    "            cur_count = int(count)\n",
    "\n",
    "    # emit the last key\n",
    "    print '{0}\\t{1}'.format(cur_key,cur_count)\n",
    "\n",
    "\n",
    "# ------------------------------ run -------------------------------- #\n",
    "if __name__ == \"__main__\":\n",
    "    sys.stderr.write(\"reporter:counter:Wordcount,Reducer,1\\n\")\n",
    "    reducer_runner()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper3.2.C.py\n",
    "!chmod a+x combiner3.2.C.py\n",
    "!chmod a+x reducer3.2.C.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /home/cloudera/W261/Spring2017/HW3/hw3.2.C-output\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.1.jar] /tmp/streamjob6133939959012997495.jar tmpDir=null\n",
      "17/01/29 02:49:04 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/29 02:49:04 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/29 02:49:04 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/01/29 02:49:05 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/01/29 02:49:05 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1484923209747_0232\n",
      "17/01/29 02:49:05 INFO impl.YarnClientImpl: Submitted application application_1484923209747_0232\n",
      "17/01/29 02:49:05 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1484923209747_0232/\n",
      "17/01/29 02:49:05 INFO mapreduce.Job: Running job: job_1484923209747_0232\n",
      "17/01/29 02:49:12 INFO mapreduce.Job: Job job_1484923209747_0232 running in uber mode : false\n",
      "17/01/29 02:49:12 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/01/29 02:49:24 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/01/29 02:49:31 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/01/29 02:49:31 INFO mapreduce.Job: Job job_1484923209747_0232 completed successfully\n",
      "17/01/29 02:49:31 INFO mapreduce.Job: Counters: 53\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4955\n",
      "\t\tFILE: Number of bytes written=375606\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910872\n",
      "\t\tHDFS: Number of bytes written=3653\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=18699\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3907\n",
      "\t\tTotal time spent by all map tasks (ms)=18699\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3907\n",
      "\t\tTotal vcore-seconds taken by all map tasks=18699\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=3907\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=19147776\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4000768\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=1348308\n",
      "\t\tMap output bytes=13424707\n",
      "\t\tMap output materialized bytes=4961\n",
      "\t\tInput split bytes=290\n",
      "\t\tCombine input records=1348308\n",
      "\t\tCombine output records=346\n",
      "\t\tReduce input groups=297\n",
      "\t\tReduce shuffle bytes=4961\n",
      "\t\tReduce input records=346\n",
      "\t\tReduce output records=297\n",
      "\t\tSpilled Records=692\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=182\n",
      "\t\tCPU time spent (ms)=5630\n",
      "\t\tPhysical memory (bytes) snapshot=720850944\n",
      "\t\tVirtual memory (bytes) snapshot=4686409728\n",
      "\t\tTotal committed heap usage (bytes)=674234368\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tWordcount\n",
      "\t\tCombiner=2\n",
      "\t\tMapper=2\n",
      "\t\tReducer=1\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50910582\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3653\n",
      "17/01/29 02:49:31 INFO streaming.StreamJob: Output directory: /home/cloudera/W261/Spring2017/HW3/hw3.2.C-output\n"
     ]
    }
   ],
   "source": [
    "# Hadoop command\n",
    "\n",
    "\n",
    "!hdfs dfs -rm -r /home/cloudera/W261/Spring2017/HW3/hw3.2.C-output\n",
    "\n",
    "!sudo hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.1.jar \\\n",
    "    -mapper /home/cloudera/W261/Spring2017/HW3/mapper3.2.C.py \\\n",
    "    -combiner /home/cloudera/W261/Spring2017/HW3/combiner3.2.C.py \\\n",
    "    -reducer /home/cloudera/W261/Spring2017/HW3/reducer3.2.C.py \\\n",
    "    -input /home/cloudera/dfs/Consumer_Complaints/Consumer_Complaints.csv  \\\n",
    "    -output /home/cloudera/W261/Spring2017/HW3/hw3.2.C-output \\\n",
    "    -numReduceTasks 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.2.C OUTPUT/ANSWER\n",
    "\n",
    "By adding a combiner (which shares the same code as the reducer), the calculations were sped up. In the end, while Hadoop makes no guarantee that a combiner would be called, two combiners were invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apr\t3431\r\n",
      "atm\t1056\r\n",
      "account\t6753\r\n",
      "advertising\t500\r\n",
      "application\t2906\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /home/cloudera/W261/Spring2017/HW3/hw3.2.C-output/part-00000 | head -5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### INSERT SCREENSHOT OF JOB TRACKER UI COUNTERS\n",
    "\n",
    "![combiner-counter](counter-32C.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting frequencies_mapper3.2.C.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile frequencies_mapper3.2.C.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32CFREQMAPPER\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "from csv import reader\n",
    "\n",
    "# ------------------------ define functions ----------------------- # \n",
    "def grouper(word,num_reducers):\n",
    "    # I guess this wouldn't really matter as it would be sent to one reducer anyway but it is better to be explicit\n",
    "    if num_reducers == 1:\n",
    "        return 0\n",
    "    elif num_reducers == 2:\n",
    "        # first group: words starting with a-m\n",
    "        if(word[0]<='m'):\n",
    "            return 0\n",
    "        # second group: words starting with n-z\n",
    "        else:\n",
    "            return 1\n",
    "    else:\n",
    "        raise ValueError(\"Grouping for only 1 and 2 reducers is supported.\")\n",
    "\n",
    "def mapper_runner(num_reducer):\n",
    "    WORD_RE = re.compile(r\"[\\w']+\")\n",
    "    for row in reader(sys.stdin):\n",
    "        try:\n",
    "            # the values in the first (id) column are integers except the header\n",
    "            is_header = int(row[0])\n",
    "            issue = row[3]\n",
    "            for word in WORD_RE.findall(issue.lower()): \n",
    "                print '{0}\\t{1}\\t{2}'.format(grouper(word,num_reducer), word, 1)\n",
    "                print '{0}\\t{1}\\t{2}'.format(grouper(word,num_reducer), '*', 1)\n",
    "        except:\n",
    "            # if can't be cast to int, it is a header, do not map it\n",
    "            pass\n",
    "\n",
    "# ----------------------------- run ------------------------------- #\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    numReducers = int(os.environ.get('NUM_PARTITIONS', '1')) #default to one reducer\n",
    "    mapper_runner(numReducers)\n",
    "# END STUDENT CODE HW32CFREQMAPPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting frequencies_reducer3.2.C.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile frequencies_reducer3.2.C.py\n",
    "#!/usr/bin/env python\n",
    "# START STUDENT CODE HW32CFREQREDUCER\n",
    "\n",
    "import sys\n",
    "\n",
    "# -------------------------- define function ------------------------ #\n",
    "\n",
    "def reducer_runner():\n",
    "    # initialize\n",
    "    cur_key = None\n",
    "    cur_count = 0\n",
    "    total = 0\n",
    "    # loop over the rows \n",
    "    for line in sys.stdin:\n",
    "        group,key,count = line.lower().strip().split()\n",
    "        # start with total counts \n",
    "        if key == '*':\n",
    "            total += int(count)\n",
    "        else: \n",
    "            if key == cur_key:\n",
    "                cur_count += int(count)\n",
    "            else:\n",
    "                if cur_key:\n",
    "                    rel_freq = float(cur_count)/total\n",
    "                    print '{0}\\t{1}\\t{2}'.format(cur_key,cur_count,rel_freq)\n",
    "                cur_key = key\n",
    "                cur_count = int(count)\n",
    "    # emit the last key\n",
    "    rel_freq = float(cur_count)/total\n",
    "    print '{0}\\t{1}\\t{2}'.format(cur_key,cur_count,rel_freq)\n",
    "\n",
    "# ---------------------------- run -------------------------------- #\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    reducer_runner()\n",
    "\n",
    "# END STUDENT CODE HW32CFREQREDUCER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x frequencies_mapper3.2.C.py\n",
    "!chmod a+x frequencies_reducer3.2.C.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /home/cloudera/W261/Spring2017/HW3/frequencies3.2.C-output\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.1.jar] /tmp/streamjob5343069697529019786.jar tmpDir=null\n",
      "17/01/29 03:07:47 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/29 03:07:48 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/29 03:07:48 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/01/29 03:07:48 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/01/29 03:07:49 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1484923209747_0235\n",
      "17/01/29 03:07:49 INFO impl.YarnClientImpl: Submitted application application_1484923209747_0235\n",
      "17/01/29 03:07:49 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1484923209747_0235/\n",
      "17/01/29 03:07:49 INFO mapreduce.Job: Running job: job_1484923209747_0235\n",
      "17/01/29 03:07:56 INFO mapreduce.Job: Job job_1484923209747_0235 running in uber mode : false\n",
      "17/01/29 03:07:56 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/01/29 03:08:07 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "17/01/29 03:08:08 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/01/29 03:08:18 INFO mapreduce.Job:  map 100% reduce 95%\n",
      "17/01/29 03:08:19 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/01/29 03:08:19 INFO mapreduce.Job: Job job_1484923209747_0235 completed successfully\n",
      "17/01/29 03:08:20 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=29604409\n",
      "\t\tFILE: Number of bytes written=59575414\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910872\n",
      "\t\tHDFS: Number of bytes written=5164\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=19255\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=9689\n",
      "\t\tTotal time spent by all map tasks (ms)=19255\n",
      "\t\tTotal time spent by all reduce tasks (ms)=9689\n",
      "\t\tTotal vcore-seconds taken by all map tasks=19255\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=9689\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=19717120\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=9921536\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=2696616\n",
      "\t\tMap output bytes=24211171\n",
      "\t\tMap output materialized bytes=29604415\n",
      "\t\tInput split bytes=290\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=175\n",
      "\t\tReduce shuffle bytes=29604415\n",
      "\t\tReduce input records=2696616\n",
      "\t\tReduce output records=174\n",
      "\t\tSpilled Records=5393232\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=434\n",
      "\t\tCPU time spent (ms)=13430\n",
      "\t\tPhysical memory (bytes) snapshot=1109413888\n",
      "\t\tVirtual memory (bytes) snapshot=4692824064\n",
      "\t\tTotal committed heap usage (bytes)=1039138816\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50910582\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5164\n",
      "17/01/29 03:08:20 INFO streaming.StreamJob: Output directory: /home/cloudera/W261/Spring2017/HW3/frequencies3.2.C-output\n"
     ]
    }
   ],
   "source": [
    "# Hadoop command\n",
    "# START STUDENT CODE HW32CFREQHADOOP\n",
    "\n",
    "!hdfs dfs -rm -r /home/cloudera/W261/Spring2017/HW3/frequencies3.2.C-output \n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D stream.num.map.output.key.fields=2 \\\n",
    "    -D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "    -D mapreduce.partition.keycomparator.options=-k2,2 \\\n",
    "    -mapper /home/cloudera/W261/Spring2017/HW3/frequencies_mapper3.2.C.py \\\n",
    "    -reducer /home/cloudera/W261/Spring2017/HW3/frequencies_reducer3.2.C.py \\\n",
    "    -input  /home/cloudera/dfs/Consumer_Complaints/Consumer_Complaints.csv \\\n",
    "    -output /home/cloudera/W261/Spring2017/HW3/frequencies3.2.C-output \\\n",
    "    -numReduceTasks 1 \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \n",
    "# END STUDENT CODE HW32CFREQHADOOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.2.C OUTPUT/ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a single reducer: \n",
    "- What are the top 50 most frequent terms in your word count analysis?    \n",
    "- Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order.    \n",
    "- Present bottom 10 tokens (least frequent items). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ top 50 -----------------------\n",
      "loan\t119630\t0.0887260180908\n",
      "collection\t72394\t0.0536924797598\n",
      "foreclosure\t70487\t0.0522781144961\n",
      "modification\t70487\t0.0522781144961\n",
      "account\t57448\t0.0426074754433\n",
      "credit\t55251\t0.0409780257923\n",
      "or\t40508\t0.0300435805469\n",
      "payments\t39993\t0.0296616203419\n",
      "escrow\t36767\t0.0272689919514\n",
      "servicing\t36767\t0.0272689919514\n",
      "report\t34903\t0.0258865185106\n",
      "incorrect\t29133\t0.0216070808747\n",
      "information\t29069\t0.0215596139754\n",
      "on\t29069\t0.0215596139754\n",
      "debt\t27874\t0.0206733179659\n",
      "closing\t19000\t0.0140917357162\n",
      "not\t18477\t0.0137038421488\n",
      "attempts\t17972\t0.0133292986469\n",
      "collect\t17972\t0.0133292986469\n",
      "cont'd\t17972\t0.0133292986469\n",
      "owed\t17972\t0.0133292986469\n",
      "and\t16448\t0.0121989931084\n",
      "management\t16205\t0.0120187672253\n",
      "opening\t16205\t0.0120187672253\n",
      "of\t13983\t0.0103707758168\n",
      "my\t10731\t0.00795886399843\n",
      "deposits\t10555\t0.00782833002548\n",
      "withdrawals\t10555\t0.00782833002548\n",
      "problems\t9484\t0.00703400113327\n",
      "application\t8868\t0.00657713222795\n",
      "communication\t8671\t0.00643102317868\n",
      "tactics\t8671\t0.00643102317868\n",
      "broker\t8625\t0.00639690634484\n",
      "mortgage\t8625\t0.00639690634484\n",
      "originator\t8625\t0.00639690634484\n",
      "to\t8401\t0.00623077219745\n",
      "unable\t8178\t0.00606537972036\n",
      "billing\t8158\t0.00605054631434\n",
      "other\t7886\t0.00584881199251\n",
      "disclosure\t7655\t0.00567748615302\n",
      "verification\t7655\t0.00567748615302\n",
      "disputes\t6938\t0.00514570854731\n",
      "reporting\t6559\t0.00486461550328\n",
      "lease\t6337\t0.00469996469649\n",
      "the\t6248\t0.00463395603972\n",
      "being\t5663\t0.00420007891372\n",
      "by\t5663\t0.00420007891372\n",
      "caused\t5663\t0.00420007891372\n",
      "funds\t5663\t0.00420007891372\n",
      "low\t5663\t0.00420007891372\n",
      "------------------------ bottom 10 -----------------------\n",
      "disclosures\t64\t4.74668992545e-05\n",
      "missing\t64\t4.74668992545e-05\n",
      "amt\t71\t5.26585913604e-05\n",
      "day\t71\t5.26585913604e-05\n",
      "checks\t75\t5.56252725638e-05\n",
      "convenience\t75\t5.56252725638e-05\n",
      "credited\t92\t6.82336676783e-05\n",
      "payment\t92\t6.82336676783e-05\n",
      "amount\t98\t7.26836894834e-05\n",
      "apply\t118\t8.75170955004e-05\n"
     ]
    }
   ],
   "source": [
    "!echo \"------------------------ top 50 -----------------------\"\n",
    "!hdfs dfs -cat /home/cloudera/W261/Spring2017/HW3/frequencies3.2.C-output/part-00000 | sort -k2,2nr | head -50 \n",
    "!echo \"------------------------ bottom 10 -----------------------\"\n",
    "!hdfs dfs -cat /home/cloudera/W261/Spring2017/HW3/frequencies3.2.C-output/part-00000 | sort -k2,2n | head -10 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3.2.1  \n",
    "Using **2 reducers**: What are the top **50 most frequent terms** in your word count analysis? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting frequencies_combiner3.2.1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile frequencies_combiner3.2.1.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "# -------------------------- define function ------------------------ #\n",
    "\n",
    "def combiner_runner():\n",
    "    # initialize\n",
    "    cur_key = None\n",
    "    cur_count = 0\n",
    "    # loop over the rows \n",
    "    for line in sys.stdin:\n",
    "        group,key,count = line.lower().strip().split()\n",
    "        # start with total counts \n",
    "        if key == cur_key:\n",
    "                cur_count += int(count)\n",
    "        else:\n",
    "            if cur_key:\n",
    "                print '{0}\\t{1}\\t{2}'.format(group,cur_key,cur_count)\n",
    "            cur_key = key\n",
    "            cur_count = int(count)\n",
    "    # emit the last key\n",
    "    print '{0}\\t{1}\\t{2}'.format(group,cur_key,cur_count)\n",
    "\n",
    "# ---------------------------- run -------------------------------- #\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    combiner_runner()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x frequencies_mapper3.2.1.py\n",
    "!chmod a+x frequencies_combiner3.2.1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /home/cloudera/W261/Spring2017/HW3/frequencies3.2.1-output\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.1.jar] /tmp/streamjob4224400894549206311.jar tmpDir=null\n",
      "17/01/31 02:04:05 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/31 02:04:05 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/31 02:04:05 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/01/31 02:04:05 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/01/31 02:04:06 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1484923209747_0374\n",
      "17/01/31 02:04:06 INFO impl.YarnClientImpl: Submitted application application_1484923209747_0374\n",
      "17/01/31 02:04:06 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1484923209747_0374/\n",
      "17/01/31 02:04:06 INFO mapreduce.Job: Running job: job_1484923209747_0374\n",
      "17/01/31 02:04:13 INFO mapreduce.Job: Job job_1484923209747_0374 running in uber mode : false\n",
      "17/01/31 02:04:13 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/01/31 02:04:25 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "17/01/31 02:04:26 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "17/01/31 02:04:27 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/01/31 02:04:36 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "17/01/31 02:04:37 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/01/31 02:04:37 INFO mapreduce.Job: Job job_1484923209747_0374 completed successfully\n",
      "17/01/31 02:04:37 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5357\n",
      "\t\tFILE: Number of bytes written=501120\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50910872\n",
      "\t\tHDFS: Number of bytes written=9497\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=24200\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=10922\n",
      "\t\tTotal time spent by all map tasks (ms)=24200\n",
      "\t\tTotal time spent by all reduce tasks (ms)=10922\n",
      "\t\tTotal vcore-seconds taken by all map tasks=24200\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=10922\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=24780800\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=11184128\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=2696616\n",
      "\t\tMap output bytes=24211171\n",
      "\t\tMap output materialized bytes=5369\n",
      "\t\tInput split bytes=290\n",
      "\t\tCombine input records=2696616\n",
      "\t\tCombine output records=328\n",
      "\t\tReduce input groups=2\n",
      "\t\tReduce shuffle bytes=5369\n",
      "\t\tReduce input records=328\n",
      "\t\tReduce output records=324\n",
      "\t\tSpilled Records=656\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=554\n",
      "\t\tCPU time spent (ms)=9910\n",
      "\t\tPhysical memory (bytes) snapshot=1178861568\n",
      "\t\tVirtual memory (bytes) snapshot=6260629504\n",
      "\t\tTotal committed heap usage (bytes)=1250426880\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50910582\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=9497\n",
      "17/01/31 02:04:37 INFO streaming.StreamJob: Output directory: /home/cloudera/W261/Spring2017/HW3/frequencies3.2.1-output\n"
     ]
    }
   ],
   "source": [
    "# Hadoop command\n",
    "\n",
    "\n",
    "!hdfs dfs -rm -r /home/cloudera/W261/Spring2017/HW3/frequencies3.2.1-output \n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D stream.num.map.output.key.fields=2 \\\n",
    "    -D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "    -D mapreduce.partition.keycomparator.options=-k2,2 \\\n",
    "    -mapper /home/cloudera/W261/Spring2017/HW3/frequencies_mapper3.2.C.py \\\n",
    "    -combiner /home/cloudera/W261/Spring2017/HW3/frequencies_combiner3.2.1.py \\\n",
    "    -reducer /home/cloudera/W261/Spring2017/HW3/frequencies_reducer3.2.C.py \\\n",
    "    -input  /home/cloudera/dfs/Consumer_Complaints/Consumer_Complaints.csv \\\n",
    "    -output /home/cloudera/W261/Spring2017/HW3/frequencies3.2.1-output \\\n",
    "    -numReduceTasks 2 \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "    -cmdenv NUM_PARTITIONS=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\r\n",
      "-rw-r--r--   1 root supergroup          0 2017-01-31 02:04 /home/cloudera/W261/Spring2017/HW3/frequencies3.2.1-output/_SUCCESS\r\n",
      "-rw-r--r--   1 root supergroup       5685 2017-01-31 02:04 /home/cloudera/W261/Spring2017/HW3/frequencies3.2.1-output/part-00000\r\n",
      "-rw-r--r--   1 root supergroup       3812 2017-01-31 02:04 /home/cloudera/W261/Spring2017/HW3/frequencies3.2.1-output/part-00001\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /home/cloudera/W261/Spring2017/HW3/frequencies3.2.1-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ top 50 -----------------------\n",
      "loan\t119630\t0.0887260180908\n",
      "collection\t72394\t0.0536924797598\n",
      "foreclosure\t70487\t0.0522781144961\n",
      "modification\t70487\t0.0522781144961\n",
      "account\t57448\t0.0426074754433\n",
      "credit\t55251\t0.0409780257923\n",
      "or\t40508\t0.0300435805469\n",
      "payments\t39993\t0.0296616203419\n",
      "escrow\t36767\t0.0272689919514\n",
      "servicing\t36767\t0.0272689919514\n",
      "report\t34903\t0.0258865185106\n",
      "incorrect\t29133\t0.0216070808747\n",
      "information\t29069\t0.0215596139754\n",
      "on\t29069\t0.0215596139754\n",
      "debt\t27874\t0.0206733179659\n",
      "closing\t19000\t0.0140917357162\n",
      "not\t18477\t0.0137038421488\n",
      "attempts\t17972\t0.0133292986469\n",
      "collect\t17972\t0.0133292986469\n",
      "cont'd\t17972\t0.0133292986469\n",
      "owed\t17972\t0.0133292986469\n",
      "and\t16448\t0.0121989931084\n",
      "management\t16205\t0.0120187672253\n",
      "opening\t16205\t0.0120187672253\n",
      "of\t13983\t0.0103707758168\n",
      "my\t10731\t0.00795886399843\n",
      "deposits\t10555\t0.00782833002548\n",
      "withdrawals\t10555\t0.00782833002548\n",
      "problems\t9484\t0.00703400113327\n",
      "application\t8868\t0.00657713222795\n",
      "communication\t8671\t0.00643102317868\n",
      "tactics\t8671\t0.00643102317868\n",
      "broker\t8625\t0.00639690634484\n",
      "mortgage\t8625\t0.00639690634484\n",
      "originator\t8625\t0.00639690634484\n",
      "to\t8401\t0.00623077219745\n",
      "unable\t8178\t0.00606537972036\n",
      "billing\t8158\t0.00605054631434\n",
      "other\t7886\t0.00584881199251\n",
      "disclosure\t7655\t0.00567748615302\n",
      "verification\t7655\t0.00567748615302\n",
      "disputes\t6938\t0.00514570854731\n",
      "reporting\t6559\t0.00486461550328\n",
      "lease\t6337\t0.00469996469649\n",
      "the\t6248\t0.00463395603972\n",
      "being\t5663\t0.00420007891372\n",
      "by\t5663\t0.00420007891372\n",
      "caused\t5663\t0.00420007891372\n",
      "funds\t5663\t0.00420007891372\n",
      "low\t5663\t0.00420007891372\n",
      "------------------------ bottom 10 -----------------------\n",
      "disclosures\t64\t4.74668992545e-05\n",
      "missing\t64\t4.74668992545e-05\n",
      "amt\t71\t5.26585913604e-05\n",
      "day\t71\t5.26585913604e-05\n",
      "checks\t75\t5.56252725638e-05\n",
      "convenience\t75\t5.56252725638e-05\n",
      "credited\t92\t6.82336676783e-05\n",
      "payment\t92\t6.82336676783e-05\n",
      "amount\t98\t7.26836894834e-05\n",
      "apply\t118\t8.75170955004e-05\n"
     ]
    }
   ],
   "source": [
    "!echo \"------------------------ top 50 -----------------------\"\n",
    "!hdfs dfs -cat /home/cloudera/W261/Spring2017/HW3/frequencies3.2.C-output/part-0000* | sort -k2,2nr | head -50 \n",
    "!echo \"------------------------ bottom 10 -----------------------\"\n",
    "!hdfs dfs -cat /home/cloudera/W261/Spring2017/HW3/frequencies3.2.C-output/part-0000* | sort -k2,2n | head -10 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Shopping Cart Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-01-29 04:25:09--  https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0\n",
      "Resolving www.dropbox.com... 162.125.65.1\n",
      "Connecting to www.dropbox.com|162.125.65.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://dl.dropboxusercontent.com/content_link/UY9M2LxUAwTEt8EojvRYXbLengRuHknPH3cS7pkQ9m8Zc59uNU5h0JKyheEiDw7f/file [following]\n",
      "--2017-01-29 04:25:09--  https://dl.dropboxusercontent.com/content_link/UY9M2LxUAwTEt8EojvRYXbLengRuHknPH3cS7pkQ9m8Zc59uNU5h0JKyheEiDw7f/file\n",
      "Resolving dl.dropboxusercontent.com... 162.125.65.6\n",
      "Connecting to dl.dropboxusercontent.com|162.125.65.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3458517 (3.3M) [text/plain]\n",
      "Saving to: “ProductPurchaseData.csv”\n",
      "\n",
      "100%[======================================>] 3,458,517   3.13M/s   in 1.1s    \n",
      "\n",
      "2017-01-29 04:25:11 (3.13 MB/s) - “ProductPurchaseData.csv” saved [3458517/3458517]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download data\n",
    "!wget https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0 -O ProductPurchaseData.csv\n",
    "!hdfs dfs -mkdir /home/cloudera/dfs/ProductPurchaseData\n",
    "!hdfs dfs -put ProductPurchaseData.csv /home/cloudera/dfs/ProductPurchaseData/ProductPurchaseData.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting frequencies_mapper3.3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile frequencies_mapper3.3.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from csv import reader\n",
    "\n",
    "# ------------------------ define functions ----------------------- # \n",
    "def grouper(word,num_reducers):\n",
    "    # I guess this wouldn't really matter as it would be sent to one reducer anyway but it is better to be explicit\n",
    "    if num_reducers == 1:\n",
    "        return 0\n",
    "    elif num_reducers == 2:\n",
    "        # first group: words starting with a-m\n",
    "        if(word[0]<='M'):\n",
    "            return 0\n",
    "        # second group: words starting with n-z\n",
    "        else:\n",
    "            return 1\n",
    "    else:\n",
    "        raise ValueError(\"Grouping for only 1 and 2 reducers is supported.\")\n",
    "\n",
    "        \n",
    "def mapper_runner(num_reducer):\n",
    "    for customer in reader(sys.stdin):\n",
    "        products = customer[0].split()\n",
    "        for product in products: \n",
    "            print '{0}\\t{1}\\t{2}'.format(grouper(product,num_reducer), product, 1)\n",
    "            print '{0}\\t{1}\\t{2}'.format(grouper(product,num_reducer), '*', 1)\n",
    "\n",
    "# ----------------------------- run ------------------------------- #\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    numReducers = int(os.environ.get('NUM_PARTITIONS', '1')) #default to one reducer\n",
    "    mapper_runner(numReducers)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting frequencies_reducer3.3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile frequencies_reducer3.3.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "# -------------------------- define function ------------------------ #\n",
    "\n",
    "def reducer_runner():\n",
    "    # initialize\n",
    "    cur_key = None\n",
    "    cur_count = 0\n",
    "    total = 0\n",
    "    # loop over the rows \n",
    "    for line in sys.stdin:\n",
    "        group,key,count = line.strip().split('\\t',3)\n",
    "        # start with total counts \n",
    "        if key == '*':\n",
    "            total += int(count)\n",
    "        else: \n",
    "            if key == cur_key:\n",
    "                cur_count += int(count)\n",
    "            else:\n",
    "                if cur_key:\n",
    "                    rel_freq = float(cur_count)/total\n",
    "                    print '{0}\\t{1}\\t{2}'.format(cur_key,cur_count,rel_freq)\n",
    "                cur_key = key\n",
    "                cur_count = int(count)\n",
    "    # emit the last key\n",
    "    rel_freq = float(cur_count)/total\n",
    "    print '{0}\\t{1}\\t{2}'.format(cur_key,cur_count,rel_freq)\n",
    "\n",
    "# ---------------------------- run -------------------------------- #\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    reducer_runner()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x frequencies_*.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /home/cloudera/W261/Spring2017/HW3/frequencies3.3-output\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.1.jar] /tmp/streamjob5100245058245541883.jar tmpDir=null\n",
      "17/01/29 14:45:22 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/29 14:45:22 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/29 14:45:23 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/01/29 14:45:23 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/01/29 14:45:23 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1484923209747_0277\n",
      "17/01/29 14:45:24 INFO impl.YarnClientImpl: Submitted application application_1484923209747_0277\n",
      "17/01/29 14:45:24 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1484923209747_0277/\n",
      "17/01/29 14:45:24 INFO mapreduce.Job: Running job: job_1484923209747_0277\n",
      "17/01/29 14:45:31 INFO mapreduce.Job: Job job_1484923209747_0277 running in uber mode : false\n",
      "17/01/29 14:45:31 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/01/29 14:45:40 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/01/29 14:45:48 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/01/29 14:45:48 INFO mapreduce.Job: Job job_1484923209747_0277 completed successfully\n",
      "17/01/29 14:45:48 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=9520606\n",
      "\t\tFILE: Number of bytes written=19407841\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3462903\n",
      "\t\tHDFS: Number of bytes written=368635\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=14177\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5757\n",
      "\t\tTotal time spent by all map tasks (ms)=14177\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5757\n",
      "\t\tTotal vcore-seconds taken by all map tasks=14177\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=5757\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=14517248\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=5895168\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=761648\n",
      "\t\tMap output bytes=7997304\n",
      "\t\tMap output materialized bytes=9520612\n",
      "\t\tInput split bytes=290\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12593\n",
      "\t\tReduce shuffle bytes=9520612\n",
      "\t\tReduce input records=761648\n",
      "\t\tReduce output records=12592\n",
      "\t\tSpilled Records=1523296\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=447\n",
      "\t\tCPU time spent (ms)=7000\n",
      "\t\tPhysical memory (bytes) snapshot=1000304640\n",
      "\t\tVirtual memory (bytes) snapshot=4691378176\n",
      "\t\tTotal committed heap usage (bytes)=974127104\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3462613\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=368635\n",
      "17/01/29 14:45:48 INFO streaming.StreamJob: Output directory: /home/cloudera/W261/Spring2017/HW3/frequencies3.3-output\n"
     ]
    }
   ],
   "source": [
    "# Hadoop command\n",
    "\n",
    "\n",
    "!hdfs dfs -rm -r /home/cloudera/W261/Spring2017/HW3/frequencies3.3-output \n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D stream.num.map.output.key.fields=3 \\\n",
    "    -D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "    -D mapreduce.partition.keycomparator.options=-k2,2 \\\n",
    "    -mapper /home/cloudera/W261/Spring2017/HW3/frequencies_mapper3.3.py \\\n",
    "    -reducer /home/cloudera/W261/Spring2017/HW3/frequencies_reducer3.3.py \\\n",
    "    -input  /home/cloudera/dfs/ProductPurchaseData/ProductPurchaseData.csv \\\n",
    "    -output /home/cloudera/W261/Spring2017/HW3/frequencies3.3-output \\\n",
    "    -numReduceTasks 1 \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "    -cmdenv NUM_PARTITIONS=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------------- number of unique items --------------------------- #\n",
      "12592\n",
      "#-------------------- top 50 highest frequency ---------------------- #\n",
      "DAI62779\t6667\t0.0175067747831\n",
      "FRO40251\t3881\t0.010191059387\n",
      "ELE17451\t3875\t0.0101753040775\n",
      "GRO73461\t3602\t0.00945843749344\n",
      "SNA80324\t3044\t0.00799319370628\n",
      "ELE32164\t2851\t0.0074863979161\n",
      "DAI75645\t2736\t0.00718442114993\n",
      "SNA45677\t2455\t0.0064465474865\n",
      "FRO31317\t2330\t0.0061183118711\n",
      "DAI85309\t2293\t0.00602115412894\n",
      "ELE26917\t2292\t0.00601852824402\n",
      "FRO80039\t2233\t0.00586360103355\n",
      "GRO21487\t2115\t0.00555374661261\n",
      "SNA99873\t2083\t0.00546971829507\n",
      "GRO59710\t2004\t0.00526227338613\n",
      "GRO71621\t1920\t0.00504169905258\n",
      "FRO85978\t1918\t0.00503644728273\n",
      "GRO30386\t1840\t0.00483162825872\n",
      "ELE74009\t1816\t0.00476860702057\n",
      "GRO56726\t1784\t0.00468457870302\n",
      "DAI63921\t1773\t0.00465569396887\n",
      "GRO46854\t1756\t0.00461105392517\n",
      "ELE66600\t1713\t0.00449814087347\n",
      "DAI83733\t1712\t0.00449551498855\n",
      "FRO32293\t1702\t0.00446925613932\n",
      "ELE66810\t1697\t0.0044561267147\n",
      "SNA55762\t1646\t0.00432220658362\n",
      "DAI22177\t1627\t0.00427231477008\n",
      "FRO78087\t1531\t0.00402022981745\n",
      "ELE99737\t1516\t0.0039808415436\n",
      "ELE34057\t1489\t0.00390994265067\n",
      "GRO94758\t1489\t0.00390994265067\n",
      "FRO35904\t1436\t0.00377077074974\n",
      "FRO53271\t1420\t0.00372875659097\n",
      "SNA93860\t1407\t0.00369462008697\n",
      "SNA90094\t1390\t0.00364998004327\n",
      "GRO38814\t1352\t0.00355019641619\n",
      "ELE56788\t1345\t0.00353181522173\n",
      "GRO61133\t1321\t0.00346879398357\n",
      "DAI88807\t1316\t0.00345566455896\n",
      "ELE74482\t1316\t0.00345566455896\n",
      "ELE59935\t1311\t0.00344253513434\n",
      "SNA96271\t1295\t0.00340052097557\n",
      "DAI43223\t1290\t0.00338739155095\n",
      "ELE91337\t1289\t0.00338476566603\n",
      "GRO15017\t1275\t0.0033480032771\n",
      "DAI31081\t1261\t0.00331124088818\n",
      "GRO81087\t1220\t0.00320357960633\n",
      "DAI22896\t1219\t0.0032009537214\n",
      "GRO85051\t1214\t0.00318782429679\n",
      "sort: write failed: standard output: Broken pipe\n",
      "sort: write error\n"
     ]
    }
   ],
   "source": [
    "!echo '#----------------- number of unique items --------------------------- #'\n",
    "!hdfs dfs -cat /home/cloudera/W261/Spring2017/HW3/frequencies3.3-output/part-00000 | wc -l\n",
    "!echo '#-------------------- top 50 highest frequency ---------------------- #'\n",
    "!hdfs dfs -cat /home/cloudera/W261/Spring2017/HW3/frequencies3.3-output/part-00000 | sort -k2,2nr | head -50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting basket_mapper3.3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile basket_mapper3.3.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "from csv import reader\n",
    "\n",
    "# ------------------------ define functions ----------------------- # \n",
    "def grouper(word,num_reducers):\n",
    "    # I guess this wouldn't really matter as it would be sent to one reducer anyway but it is better to be explicit\n",
    "    if num_reducers == 1:\n",
    "        return 0\n",
    "    elif num_reducers == 2:\n",
    "        # first group: words starting with a-m\n",
    "        if(word[0]<='M'):\n",
    "            return 0\n",
    "        # second group: words starting with n-z\n",
    "        else:\n",
    "            return 1\n",
    "    else:\n",
    "        raise ValueError(\"Grouping for only 1 and 2 reducers is supported.\")\n",
    "\n",
    "        \n",
    "def mapper_runner(num_reducer):\n",
    "    max_basket_size = 0\n",
    "    WORD_RE = re.compile(r\"[\\w']+\")\n",
    "    for customer in reader(sys.stdin):\n",
    "        basket_size = len(WORD_RE.findall(customer[0]))\n",
    "        if basket_size > max_basket_size:\n",
    "            max_basket_size = basket_size\n",
    "    print max_basket_size\n",
    "            \n",
    "        \n",
    "\n",
    "# ----------------------------- run ------------------------------- #\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    numReducers = int(os.environ.get('NUM_PARTITIONS', '1')) #default to one reducer\n",
    "    mapper_runner(numReducers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x basket_mapper3.3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /home/cloudera/W261/Spring2017/HW3/basket3.3-output\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.1.jar] /tmp/streamjob5253531340811941659.jar tmpDir=null\n",
      "17/01/29 13:04:22 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/29 13:04:22 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/29 13:04:23 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/01/29 13:04:23 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/01/29 13:04:23 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1484923209747_0259\n",
      "17/01/29 13:04:23 INFO impl.YarnClientImpl: Submitted application application_1484923209747_0259\n",
      "17/01/29 13:04:24 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1484923209747_0259/\n",
      "17/01/29 13:04:24 INFO mapreduce.Job: Running job: job_1484923209747_0259\n",
      "17/01/29 13:04:31 INFO mapreduce.Job: Job job_1484923209747_0259 running in uber mode : false\n",
      "17/01/29 13:04:31 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/01/29 13:04:41 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/01/29 13:04:41 INFO mapreduce.Job: Job job_1484923209747_0259 completed successfully\n",
      "17/01/29 13:04:41 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=242716\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3462903\n",
      "\t\tHDFS: Number of bytes written=8\n",
      "\t\tHDFS: Number of read operations=10\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=14950\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=14950\n",
      "\t\tTotal vcore-seconds taken by all map tasks=14950\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=15308800\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=2\n",
      "\t\tInput split bytes=290\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=154\n",
      "\t\tCPU time spent (ms)=1960\n",
      "\t\tPhysical memory (bytes) snapshot=350146560\n",
      "\t\tVirtual memory (bytes) snapshot=3108225024\n",
      "\t\tTotal committed heap usage (bytes)=307232768\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3462613\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=8\n",
      "17/01/29 13:04:41 INFO streaming.StreamJob: Output directory: /home/cloudera/W261/Spring2017/HW3/basket3.3-output\n"
     ]
    }
   ],
   "source": [
    "# Hadoop command\n",
    "\n",
    "\n",
    "!hdfs dfs -rm -r /home/cloudera/W261/Spring2017/HW3/basket3.3-output \n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -mapper /home/cloudera/W261/Spring2017/HW3/basket_mapper3.3.py \\\n",
    "    -input  /home/cloudera/dfs/ProductPurchaseData/ProductPurchaseData.csv \\\n",
    "    -output /home/cloudera/W261/Spring2017/HW3/basket3.3-output \\\n",
    "    -numReduceTasks 0 \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "    -cmdenv NUM_PARTITIONS=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- largest basket size -------------------- \n",
      "37\t\n"
     ]
    }
   ],
   "source": [
    "!echo \"----------------- largest basket size -------------------- \"\n",
    "!hdfs dfs -cat /home/cloudera/W261/Spring2017/HW3/basket3.3-output/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Using 2 reducers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /home/cloudera/W261/Spring2017/HW3/frequencies3.3.1-output\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.1.jar] /tmp/streamjob5941898849205375092.jar tmpDir=null\n",
      "17/01/31 01:53:17 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/31 01:53:17 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/31 01:53:18 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/01/31 01:53:18 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/01/31 01:53:18 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1484923209747_0372\n",
      "17/01/31 01:53:18 INFO impl.YarnClientImpl: Submitted application application_1484923209747_0372\n",
      "17/01/31 01:53:18 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1484923209747_0372/\n",
      "17/01/31 01:53:18 INFO mapreduce.Job: Running job: job_1484923209747_0372\n",
      "17/01/31 01:53:25 INFO mapreduce.Job: Job job_1484923209747_0372 running in uber mode : false\n",
      "17/01/31 01:53:25 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/01/31 01:53:35 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/01/31 01:53:45 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/01/31 01:53:45 INFO mapreduce.Job: Job job_1484923209747_0372 completed successfully\n",
      "17/01/31 01:53:45 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=9520612\n",
      "\t\tFILE: Number of bytes written=19530958\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3462903\n",
      "\t\tHDFS: Number of bytes written=368077\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=15036\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=14637\n",
      "\t\tTotal time spent by all map tasks (ms)=15036\n",
      "\t\tTotal time spent by all reduce tasks (ms)=14637\n",
      "\t\tTotal vcore-seconds taken by all map tasks=15036\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=14637\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=15396864\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=14988288\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=761648\n",
      "\t\tMap output bytes=7997304\n",
      "\t\tMap output materialized bytes=9520624\n",
      "\t\tInput split bytes=290\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12594\n",
      "\t\tReduce shuffle bytes=9520624\n",
      "\t\tReduce input records=761648\n",
      "\t\tReduce output records=12592\n",
      "\t\tSpilled Records=1523296\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=568\n",
      "\t\tCPU time spent (ms)=9360\n",
      "\t\tPhysical memory (bytes) snapshot=1272745984\n",
      "\t\tVirtual memory (bytes) snapshot=6244089856\n",
      "\t\tTotal committed heap usage (bytes)=1333788672\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3462613\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=368077\n",
      "17/01/31 01:53:45 INFO streaming.StreamJob: Output directory: /home/cloudera/W261/Spring2017/HW3/frequencies3.3.1-output\n"
     ]
    }
   ],
   "source": [
    "# Hadoop command\n",
    "\n",
    "\n",
    "!hdfs dfs -rm -r /home/cloudera/W261/Spring2017/HW3/frequencies3.3.1-output \n",
    "\n",
    "!hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "    -D stream.num.map.output.key.fields=3 \\\n",
    "    -D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "    -D mapreduce.partition.keycomparator.options=-k2,2 \\\n",
    "    -mapper /home/cloudera/W261/Spring2017/HW3/frequencies_mapper3.3.py \\\n",
    "    -reducer /home/cloudera/W261/Spring2017/HW3/frequencies_reducer3.3.py \\\n",
    "    -input  /home/cloudera/dfs/ProductPurchaseData/ProductPurchaseData.csv \\\n",
    "    -output /home/cloudera/W261/Spring2017/HW3/frequencies3.3.1-output \\\n",
    "    -numReduceTasks 2 \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "    -cmdenv NUM_PARTITIONS=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------------- number of unique items --------------------------- #\n",
      "12592\n",
      "#-------------------- top 50 highest frequency ---------------------- #\n",
      "DAI62779\t6667\t0.0216475095785\n",
      "FRO40251\t3881\t0.0126014676278\n",
      "ELE17451\t3875\t0.0125819858432\n",
      "GRO73461\t3602\t0.0116955646471\n",
      "SNA80324\t3044\t0.0417879303718\n",
      "ELE32164\t2851\t0.00925709461653\n",
      "DAI75645\t2736\t0.00888369374635\n",
      "SNA45677\t2455\t0.0337021580364\n",
      "FRO31317\t2330\t0.00756542632638\n",
      "DAI85309\t2293\t0.00744528865511\n",
      "ELE26917\t2292\t0.00744204169102\n",
      "FRO80039\t2233\t0.00725047080979\n",
      "GRO21487\t2115\t0.00686732904734\n",
      "SNA99873\t2083\t0.0285953544561\n",
      "GRO59710\t2004\t0.00650691603351\n",
      "GRO71621\t1920\t0.00623417105007\n",
      "FRO85978\t1918\t0.00622767712189\n",
      "GRO30386\t1840\t0.00597441392298\n",
      "ELE74009\t1816\t0.00589648678486\n",
      "GRO56726\t1784\t0.00579258393402\n",
      "DAI63921\t1773\t0.00575686732905\n",
      "GRO46854\t1756\t0.00570166893954\n",
      "ELE66600\t1713\t0.00556204948373\n",
      "DAI83733\t1712\t0.00555880251964\n",
      "FRO32293\t1702\t0.00552633287876\n",
      "ELE66810\t1697\t0.00551009805832\n",
      "SNA55762\t1646\t0.022596233046\n",
      "DAI22177\t1627\t0.00528281057212\n",
      "FRO78087\t1531\t0.00497110201961\n",
      "ELE99737\t1516\t0.00492239755828\n",
      "ELE34057\t1489\t0.00483472952789\n",
      "GRO94758\t1489\t0.00483472952789\n",
      "FRO35904\t1436\t0.0046626404312\n",
      "FRO53271\t1420\t0.00461068900578\n",
      "SNA93860\t1407\t0.0193152490253\n",
      "SNA90094\t1390\t0.0190818735929\n",
      "GRO38814\t1352\t0.00438989544776\n",
      "ELE56788\t1345\t0.00436716669914\n",
      "GRO61133\t1321\t0.00428923956101\n",
      "DAI88807\t1316\t0.00427300474057\n",
      "ELE74482\t1316\t0.00427300474057\n",
      "ELE59935\t1311\t0.00425676992012\n",
      "SNA96271\t1295\t0.0177777167646\n",
      "DAI43223\t1290\t0.00418858367426\n",
      "ELE91337\t1289\t0.00418533671018\n",
      "GRO15017\t1275\t0.00413987921294\n",
      "DAI31081\t1261\t0.0040944217157\n",
      "GRO81087\t1220\t0.00396129618806\n",
      "DAI22896\t1219\t0.00395804922398\n",
      "GRO85051\t1214\t0.00394181440353\n",
      "sort: write failed: standard output: Broken pipe\n",
      "sort: write error\n"
     ]
    }
   ],
   "source": [
    "!echo '#----------------- number of unique items --------------------------- #'\n",
    "!hdfs dfs -cat /home/cloudera/W261/Spring2017/HW3/frequencies3.3.1-output/part-0000* | wc -l\n",
    "!echo '#-------------------- top 50 highest frequency ---------------------- #'\n",
    "!hdfs dfs -cat /home/cloudera/W261/Spring2017/HW3/frequencies3.3.1-output/part-0000* | sort -k2,2nr | head -50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the largest basket does not require a reducer, hence the code and results are the same as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Pairs\n",
    "\n",
    "Suppose we want to recommend new products to the customer based on the products they\n",
    "have already browsed on the online website. Write a map-reduce program \n",
    "to find products which are frequently browsed together. Fix the support count (cooccurence count) to s = 100 \n",
    "(i.e. product pairs need to occur together at least 100 times to be considered frequent) \n",
    "and find pairs of items (sometimes referred to itemsets of size 2 in association rule mining) that have a support count of 100 or more.\n",
    "\n",
    "List the top 50 product pairs with corresponding support count (aka frequency), and relative frequency or support (number of records where they coccur, the number of records where they coccur/the number of baskets in the dataset)  in decreasing order of support  for frequent (100>count) itemsets of size 2. \n",
    "\n",
    "Use the Pairs pattern (lecture 3)  to  extract these frequent itemsets of size 2. Free free to use combiners if they bring value. Instrument your code with counters for count the number of times your mapper, combiner and reducers are called.  \n",
    "\n",
    "Please output records of the following form for the top 50 pairs (itemsets of size 2): \n",
    "\n",
    "      item1, item2, support count, support\n",
    "\n",
    "\n",
    "\n",
    "Fix the ordering of the pairs lexicographically (left to right), \n",
    "and break ties in support (between pairs, if any exist) \n",
    "by taking the first ones in lexicographically increasing order. \n",
    "\n",
    "Report  the compute time for the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1:\n",
    "\n",
    "Calculate frequency of each unique pair. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pairs_mapper3.4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pairs_mapper3.4.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from csv import reader\n",
    "\n",
    "# ------------------------ define functions ----------------------- # \n",
    "def grouper(word,num_reducers):\n",
    "    # I guess this wouldn't really matter as it would be sent to one reducer anyway but it is better to be explicit\n",
    "    if num_reducers == 1:\n",
    "        return 0\n",
    "    elif num_reducers == 2:\n",
    "        # first group: words starting with a-m\n",
    "        if(word[0]<='M'):\n",
    "            return 0\n",
    "        # second group: words starting with n-z\n",
    "        else:\n",
    "            return 1\n",
    "    else:\n",
    "        raise ValueError(\"Grouping for only 1 and 2 reducers is supported.\")\n",
    "        \n",
    "def get_pairs(item_list):\n",
    "    for i, product in enumerate(item_list):\n",
    "        for j in range(i+1, len(item_list)):\n",
    "            yield [product, item_list[j]]\n",
    "\n",
    "        \n",
    "def mapper_runner(num_reducer):\n",
    "    \n",
    "    for customer in reader(sys.stdin):\n",
    "        # do not take repetitions into account\n",
    "        basket = customer[0].split()\n",
    "        products = sorted(list(set(basket)))\n",
    "        # get all the unique pairs,  \n",
    "        pairs = list(get_pairs(products))\n",
    "        for pair in pairs:\n",
    "            print '{0}\\t{1}\\t{2}\\t{3}'.format(grouper(pair[0],num_reducer),pair[0], pair[1],1) \n",
    "        # count total number of baskets    \n",
    "        print '{0}\\t{1}\\t{2}\\t{3}'.format(grouper('*',num_reducer),'*', '*',1)\n",
    "       \n",
    "\n",
    "    # ----------------------------- run ------------------------------- #\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.stderr.write(\"reporter:counter:Pairs analysis,Mapper,1\\n\")\n",
    "    numReducers = int(os.environ.get('NUM_PARTITIONS', '1')) #default to one reducer\n",
    "    mapper_runner(numReducers)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pairs_reducer3.4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pairs_reducer3.4.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "# -------------------------- define function ------------------------ #\n",
    "\n",
    "\n",
    "def count_emitter(key,count):\n",
    "    print '{0}\\t{1}\\t{2}'.format(key[0],key[1],count)\n",
    "\n",
    "def reducer_runner():\n",
    "    # initialize\n",
    "    cur_key = None\n",
    "    cur_count = 0\n",
    "    # loop over the rows \n",
    "    for line in sys.stdin:\n",
    "        group,pair0,pair1,count = line.strip().split('\\t',4)\n",
    "        key = [pair0,pair1]\n",
    "        if key == cur_key:\n",
    "            cur_count += int(count)\n",
    "        else:\n",
    "            if cur_key:\n",
    "                count_emitter(cur_key,cur_count)\n",
    "            cur_key = key\n",
    "            cur_count = int(count)\n",
    "    # emit the last key \n",
    "    count_emitter(cur_key,cur_count)\n",
    "\n",
    "# ---------------------------- run -------------------------------- #\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.stderr.write(\"reporter:counter:Pairs analysis,Reducer,1\\n\")\n",
    "    reducer_runner()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x *3.4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /home/cloudera/W261/Spring2017/HW3/freq_pair.3.4-output\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.1.jar] /tmp/streamjob8769374255391815545.jar tmpDir=null\n",
      "17/01/31 01:31:12 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/31 01:31:12 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/31 01:31:12 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/01/31 01:31:13 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/01/31 01:31:13 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1484923209747_0366\n",
      "17/01/31 01:31:13 INFO impl.YarnClientImpl: Submitted application application_1484923209747_0366\n",
      "17/01/31 01:31:13 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1484923209747_0366/\n",
      "17/01/31 01:31:13 INFO mapreduce.Job: Running job: job_1484923209747_0366\n",
      "17/01/31 01:31:19 INFO mapreduce.Job: Job job_1484923209747_0366 running in uber mode : false\n",
      "17/01/31 01:31:19 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/01/31 01:31:32 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "17/01/31 01:31:33 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "17/01/31 01:31:34 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/01/31 01:31:44 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "17/01/31 01:31:46 INFO mapreduce.Job:  map 100% reduce 89%\n",
      "17/01/31 01:31:49 INFO mapreduce.Job:  map 100% reduce 96%\n",
      "17/01/31 01:31:51 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/01/31 01:31:51 INFO mapreduce.Job: Job job_1484923209747_0366 completed successfully\n",
      "17/01/31 01:31:51 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=61127358\n",
      "\t\tFILE: Number of bytes written=122743518\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3462903\n",
      "\t\tHDFS: Number of bytes written=17581645\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=23603\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=25079\n",
      "\t\tTotal time spent by all map tasks (ms)=23603\n",
      "\t\tTotal time spent by all reduce tasks (ms)=25079\n",
      "\t\tTotal vcore-seconds taken by all map tasks=23603\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=25079\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=24169472\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=25680896\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=2565115\n",
      "\t\tMap output bytes=55997116\n",
      "\t\tMap output materialized bytes=61127370\n",
      "\t\tInput split bytes=290\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=877096\n",
      "\t\tReduce shuffle bytes=61127370\n",
      "\t\tReduce input records=2565115\n",
      "\t\tReduce output records=877096\n",
      "\t\tSpilled Records=5130230\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=835\n",
      "\t\tCPU time spent (ms)=20740\n",
      "\t\tPhysical memory (bytes) snapshot=1499115520\n",
      "\t\tVirtual memory (bytes) snapshot=6257446912\n",
      "\t\tTotal committed heap usage (bytes)=1566572544\n",
      "\tPairs analysis\n",
      "\t\tMapper=2\n",
      "\t\tReducer=2\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3462613\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=17581645\n",
      "17/01/31 01:31:51 INFO streaming.StreamJob: Output directory: /home/cloudera/W261/Spring2017/HW3/freq_pair.3.4-output\n",
      "\n",
      "real\t0m41.822s\n",
      "user\t0m5.369s\n",
      "sys\t0m0.492s\n"
     ]
    }
   ],
   "source": [
    "# Hadoop command\n",
    "\n",
    "\n",
    "!hdfs dfs -rm -r /home/cloudera/W261/Spring2017/HW3/freq_pair.3.4-output \n",
    "\n",
    "!time hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D stream.num.map.output.key.fields=3 \\\n",
    "    -D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "    -D mapreduce.partition.keycomparator.options=-k2,2 \\\n",
    "    -mapper /home/cloudera/W261/Spring2017/HW3/pairs_mapper3.4.py \\\n",
    "    -reducer /home/cloudera/W261/Spring2017/HW3/pairs_reducer3.4.py \\\n",
    "    -input  /home/cloudera/dfs/ProductPurchaseData/ProductPurchaseData.csv \\\n",
    "    -output /home/cloudera/W261/Spring2017/HW3/freq_pair.3.4-output \\\n",
    "    -numReduceTasks 2 \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "    -cmdenv NUM_PARTITIONS=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\r\n",
      "-rw-r--r--   1 root supergroup          0 2017-01-31 01:31 /home/cloudera/W261/Spring2017/HW3/freq_pair.3.4-output/_SUCCESS\r\n",
      "-rw-r--r--   1 root supergroup   16871458 2017-01-31 01:31 /home/cloudera/W261/Spring2017/HW3/freq_pair.3.4-output/part-00000\r\n",
      "-rw-r--r--   1 root supergroup     710187 2017-01-31 01:31 /home/cloudera/W261/Spring2017/HW3/freq_pair.3.4-output/part-00001\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /home/cloudera/W261/Spring2017/HW3/freq_pair.3.4-output # not the best grouping!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2:\n",
    "Sort and calculate relative frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pairs_reducer3.41.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pairs_reducer3.41.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "# -------------------------- define function ------------------------ #\n",
    "\n",
    "\n",
    "def count_emitter(key,count,total,s=100):\n",
    "    if count > s:\n",
    "        print '{0}\\t{1}\\t{2}\\t{3}'.format(key[0],key[1],count,float(count)/total)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "def reducer_runner():\n",
    "    # initialize\n",
    "    num_baskets = 0\n",
    "    # loop over the rows \n",
    "    for basket in sys.stdin:\n",
    "        pair0,pair1,count = basket.strip().split('\\t',3)\n",
    "        count = int(count)\n",
    "        key = [pair0,pair1]\n",
    "        if key == ['*','*']:\n",
    "            total = int(count)\n",
    "        else:\n",
    "            count_emitter(key,count,total)\n",
    "\n",
    "# ---------------------------- run -------------------------------- #\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.stderr.write(\"reporter:counter:Pairs analysis,Reducer 2,1\\n\")\n",
    "    reducer_runner()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x pairs_reducer3.41.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /home/cloudera/W261/Spring2017/HW3/freq_pair.3.41-output\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.1.jar] /tmp/streamjob2883337926454385287.jar tmpDir=null\n",
      "17/01/31 01:32:21 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/31 01:32:21 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/31 01:32:21 INFO mapred.FileInputFormat: Total input paths to process : 2\n",
      "17/01/31 01:32:22 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "17/01/31 01:32:22 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1484923209747_0367\n",
      "17/01/31 01:32:22 INFO impl.YarnClientImpl: Submitted application application_1484923209747_0367\n",
      "17/01/31 01:32:22 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1484923209747_0367/\n",
      "17/01/31 01:32:22 INFO mapreduce.Job: Running job: job_1484923209747_0367\n",
      "17/01/31 01:32:29 INFO mapreduce.Job: Job job_1484923209747_0367 running in uber mode : false\n",
      "17/01/31 01:32:30 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/01/31 01:32:41 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "17/01/31 01:32:42 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "17/01/31 01:32:43 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/01/31 01:32:50 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/01/31 01:32:50 INFO mapreduce.Job: Job job_1484923209747_0367 completed successfully\n",
      "17/01/31 01:32:50 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=20212939\n",
      "\t\tFILE: Number of bytes written=40915275\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=17586188\n",
      "\t\tHDFS: Number of bytes written=50868\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=27185\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=6695\n",
      "\t\tTotal time spent by all map tasks (ms)=27185\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6695\n",
      "\t\tTotal vcore-seconds taken by all map tasks=27185\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=6695\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=27837440\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=6855680\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=877096\n",
      "\t\tMap output records=877096\n",
      "\t\tMap output bytes=18458741\n",
      "\t\tMap output materialized bytes=20212951\n",
      "\t\tInput split bytes=447\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=877096\n",
      "\t\tReduce shuffle bytes=20212951\n",
      "\t\tReduce input records=877096\n",
      "\t\tReduce output records=1311\n",
      "\t\tSpilled Records=1754192\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=361\n",
      "\t\tCPU time spent (ms)=8800\n",
      "\t\tPhysical memory (bytes) snapshot=1293783040\n",
      "\t\tVirtual memory (bytes) snapshot=6249598976\n",
      "\t\tTotal committed heap usage (bytes)=1263534080\n",
      "\tPairs analysis\n",
      "\t\tReducer 2=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=17585741\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=50868\n",
      "17/01/31 01:32:50 INFO streaming.StreamJob: Output directory: /home/cloudera/W261/Spring2017/HW3/freq_pair.3.41-output\n",
      "\n",
      "real\t0m31.862s\n",
      "user\t0m4.733s\n",
      "sys\t0m0.449s\n"
     ]
    }
   ],
   "source": [
    "# Hadoop command\n",
    "\n",
    "!hdfs dfs -rm -r /home/cloudera/W261/Spring2017/HW3/freq_pair.3.41-output \n",
    "\n",
    "!time hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "    -D stream.num.map.output.key.fields=4 \\\n",
    "    -D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "    -D mapreduce.partition.keycomparator.options=-k3,3nr \\\n",
    "    -mapper 'cat' \\\n",
    "    -reducer /home/cloudera/W261/Spring2017/HW3/pairs_reducer3.41.py \\\n",
    "    -input  /home/cloudera/W261/Spring2017/HW3/freq_pair.3.4-output/part-0000* \\\n",
    "    -output /home/cloudera/W261/Spring2017/HW3/freq_pair.3.41-output \\\n",
    "    -numReduceTasks 1 \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAI62779\tELE17451\t1592\t0.0511880646925\r\n",
      "FRO40251\tSNA80324\t1412\t0.0454004694383\r\n",
      "DAI75645\tFRO40251\t1254\t0.0403202469374\r\n",
      "FRO40251\tGRO85051\t1213\t0.0390019613517\r\n",
      "DAI62779\tGRO73461\t1139\t0.0366226166361\r\n",
      "DAI75645\tSNA80324\t1130\t0.0363332368734\r\n",
      "DAI62779\tFRO40251\t1070\t0.0344040384554\r\n",
      "DAI62779\tSNA80324\t923\t0.0296775023311\r\n",
      "DAI62779\tDAI85309\t918\t0.0295167357963\r\n",
      "ELE32164\tGRO59710\t911\t0.0292916626475\r\n",
      "DAI62779\tDAI75645\t882\t0.0283592167454\r\n",
      "FRO40251\tGRO73461\t882\t0.0283592167454\r\n",
      "DAI62779\tELE92920\t877\t0.0281984502106\r\n",
      "FRO40251\tFRO92469\t835\t0.026848011318\r\n",
      "DAI62779\tELE32164\t832\t0.0267515513971\r\n",
      "DAI75645\tGRO73461\t712\t0.0228931545609\r\n",
      "DAI43223\tELE32164\t711\t0.022861001254\r\n",
      "DAI62779\tGRO30386\t709\t0.02279669464\r\n",
      "ELE17451\tFRO40251\t697\t0.0224108549564\r\n",
      "DAI85309\tELE99737\t659\t0.0211890292917\r\n",
      "DAI62779\tELE26917\t650\t0.020899649529\r\n",
      "GRO21487\tGRO73461\t631\t0.0202887366966\r\n",
      "DAI62779\tSNA45677\t604\t0.0194205974084\r\n",
      "ELE17451\tSNA80324\t597\t0.0191955242597\r\n",
      "DAI62779\tGRO71621\t595\t0.0191312176457\r\n",
      "DAI62779\tSNA55762\t593\t0.0190669110318\r\n",
      "DAI62779\tDAI83733\t586\t0.018841837883\r\n",
      "ELE17451\tGRO73461\t580\t0.0186489180412\r\n",
      "GRO73461\tSNA80324\t562\t0.0180701585158\r\n",
      "DAI62779\tGRO59710\t561\t0.0180380052088\r\n",
      "DAI62779\tFRO80039\t550\t0.0176843188322\r\n",
      "DAI75645\tELE17451\t547\t0.0175878589113\r\n",
      "DAI62779\tSNA93860\t537\t0.0172663258416\r\n",
      "DAI55148\tDAI62779\t526\t0.016912639465\r\n",
      "DAI43223\tGRO59710\t512\t0.0164624931674\r\n",
      "ELE17451\tELE32164\t511\t0.0164303398605\r\n",
      "DAI62779\tSNA18336\t506\t0.0162695733256\r\n",
      "ELE32164\tGRO73461\t486\t0.0156265071863\r\n",
      "DAI85309\tELE17451\t482\t0.0154978939584\r\n",
      "DAI62779\tFRO78087\t482\t0.0154978939584\r\n",
      "DAI62779\tGRO94758\t479\t0.0154014340375\r\n",
      "DAI62779\tGRO21487\t471\t0.0151442075817\r\n",
      "GRO85051\tSNA80324\t471\t0.0151442075817\r\n",
      "ELE17451\tGRO30386\t468\t0.0150477476608\r\n",
      "FRO85978\tSNA95666\t463\t0.014886981126\r\n",
      "DAI62779\tFRO19221\t462\t0.014854827819\r\n",
      "DAI62779\tGRO46854\t461\t0.0148226745121\r\n",
      "DAI43223\tDAI62779\t459\t0.0147583678981\r\n",
      "ELE92920\tSNA18336\t455\t0.0146297546703\r\n",
      "DAI88079\tFRO40251\t446\t0.0143403749076\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /home/cloudera/W261/Spring2017/HW3/freq_pair.3.41-output/part-00000 | head -50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computational setup:\n",
    "Single computer on Cloudera VM (CentOS 6), 2 cores and 8 GB RAM given to VM.\n",
    "\n",
    "The first job that emitted the pairs and calculated their frequencies called 2 mappers and 2 reducers while the second job which calculated the relative frequencies and sorted the output called only one reducer and used only an identity mapper.\n",
    "\n",
    "First job required 41 seconds, the second 35 seconds to run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.5\"></a>\n",
    "##  Stripes\n",
    "Repeat the pairs exercise using the stripes design pattern for finding cooccuring pairs.\n",
    "\n",
    "Report  the compute times for stripes job versus the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts. Discuss the differences in these counts between the Pairs and Stripes jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stripes_mapper35.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stripes_mapper35.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from csv import reader\n",
    "from collections import defaultdict\n",
    "from itertools import repeat\n",
    "\n",
    "# ------------------------ define functions ----------------------- # \n",
    "def grouper(word,num_reducers):\n",
    "    # I guess this wouldn't really matter as it would be sent to one reducer anyway but it is better to be explicit\n",
    "    if num_reducers == 1:\n",
    "        return 0\n",
    "    elif num_reducers == 2:\n",
    "        # first group: words starting with a-m\n",
    "        if(word[0]<='M'):\n",
    "            return 0\n",
    "        # second group: words starting with n-z\n",
    "        else:\n",
    "            return 1\n",
    "    else:\n",
    "        raise ValueError(\"Grouping for only 1 and 2 reducers is supported.\")\n",
    "        \n",
    "def mapper_runner(num_reducer):\n",
    "    \n",
    "    for customer in reader(sys.stdin):\n",
    "        # do not take repetitions into account\n",
    "        basket = customer[0].split()\n",
    "        products = sorted(list(set(basket)))\n",
    "        stripe = {}\n",
    "        # get all the unique pairs,  \n",
    "        for i,product in enumerate(products):\n",
    "            # generate stripe:\n",
    "            #    1: get all of the other products \n",
    "            other_products = products[(i+1):]\n",
    "            #    2: create a container for products and counts\n",
    "            item = [None]*(len(other_products)*2)\n",
    "            #    3: associate products with counts of 1\n",
    "            item[::2] = other_products\n",
    "            item[1::2] = repeat(1,len(other_products))\n",
    "            item = dict(map(None, *[iter(item)]*2))\n",
    "            #    4: place them in a dictionary to the product key\n",
    "            stripe[product] = item\n",
    "            print '[{0}-{1}]\\t{1}\\t{2}'.format(grouper(product,num_reducer),product,stripe[product]) \n",
    "            # stop the loop when only the last item remains in the basket\n",
    "            if len(stripe[product]) == 1:\n",
    "                break\n",
    "        # count total number of baskets    \n",
    "        print '[{0}-{1}]\\t{1}\\t{2}'.format(grouper('*',num_reducer),'*',{'*':1})\n",
    "       \n",
    "\n",
    "    # ----------------------------- run ------------------------------- #\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.stderr.write(\"reporter:counter:Stripes analysis,Mapper,1\\n\")\n",
    "    numReducers = int(os.environ.get('NUM_PARTITIONS', '1')) #default to one reducer\n",
    "    mapper_runner(numReducers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stripes_reducer35.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stripes_reducer35.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import ast\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "# ----------------------------- define functions --------------------- #\n",
    "\n",
    "def add_dictionaries(dict1, dict2):\n",
    "    new_dict = {}\n",
    "    for k in itertools.chain(dict1.keys(), dict2.keys()):\n",
    "        new_dict[k] = dict1.get(k, 0)+dict2.get(k, 0)\n",
    "    return new_dict\n",
    "\n",
    "def reducer_runner():\n",
    "    # initialize\n",
    "    cur_key = None\n",
    "    cur_pair_dict = defaultdict(int)\n",
    "    \n",
    "    # loop over the rows \n",
    "    for line in sys.stdin:\n",
    "        try:\n",
    "            line = line.strip().split('\\t',3)\n",
    "            group,key,pair_dict = line\n",
    "            # convert the print statement to dictionary\n",
    "            pair_dict = ast.literal_eval(pair_dict)\n",
    "            # merge and sum dictionaries if subsequent keys are the same\n",
    "            if key == cur_key:\n",
    "                cur_pair_dict = add_dictionaries(cur_pair_dict,pair_dict)  \n",
    "            # if a new key comes, emit the current one and set the current key/dictionary to the new key/dictionary\n",
    "            else:\n",
    "                if cur_key:\n",
    "                    print '{0}\\t{1}'.format(cur_key,cur_pair_dict)\n",
    "                cur_key = key\n",
    "                cur_pair_dict = pair_dict \n",
    "        except ValueError:\n",
    "            sys.stderr.write(\"reporter:counter:Stripes analysis,ValueError,1\\n\")\n",
    "        # emit the last key \n",
    "    print '{0}\\t{1}'.format(cur_key,cur_pair_dict)\n",
    "\n",
    "# ---------------------------- run -------------------------------- #\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.stderr.write(\"reporter:counter:Stripes analysis,Reducer,1\\n\")\n",
    "    reducer_runner()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x stripes_*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /home/cloudera/W261/Spring2017/HW3/freq_stripes.3.5-output\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.1.jar] /tmp/streamjob2134867133890228098.jar tmpDir=null\n",
      "17/01/31 01:24:21 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/31 01:24:21 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/31 01:24:21 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/01/31 01:24:21 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/01/31 01:24:22 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1484923209747_0364\n",
      "17/01/31 01:24:22 INFO impl.YarnClientImpl: Submitted application application_1484923209747_0364\n",
      "17/01/31 01:24:22 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1484923209747_0364/\n",
      "17/01/31 01:24:22 INFO mapreduce.Job: Running job: job_1484923209747_0364\n",
      "17/01/31 01:24:28 INFO mapreduce.Job: Job job_1484923209747_0364 running in uber mode : false\n",
      "17/01/31 01:24:28 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/01/31 01:24:40 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/01/31 01:24:52 INFO mapreduce.Job:  map 100% reduce 35%\n",
      "17/01/31 01:24:54 INFO mapreduce.Job:  map 100% reduce 70%\n",
      "17/01/31 01:24:55 INFO mapreduce.Job:  map 100% reduce 72%\n",
      "17/01/31 01:24:57 INFO mapreduce.Job:  map 100% reduce 73%\n",
      "17/01/31 01:24:59 INFO mapreduce.Job:  map 100% reduce 74%\n",
      "17/01/31 01:25:01 INFO mapreduce.Job:  map 100% reduce 75%\n",
      "17/01/31 01:25:02 INFO mapreduce.Job:  map 100% reduce 76%\n",
      "17/01/31 01:25:05 INFO mapreduce.Job:  map 100% reduce 77%\n",
      "17/01/31 01:25:07 INFO mapreduce.Job:  map 100% reduce 78%\n",
      "17/01/31 01:25:10 INFO mapreduce.Job:  map 100% reduce 79%\n",
      "17/01/31 01:25:11 INFO mapreduce.Job:  map 100% reduce 80%\n",
      "17/01/31 01:25:13 INFO mapreduce.Job:  map 100% reduce 81%\n",
      "17/01/31 01:25:14 INFO mapreduce.Job:  map 100% reduce 82%\n",
      "17/01/31 01:25:16 INFO mapreduce.Job:  map 100% reduce 83%\n",
      "17/01/31 01:25:17 INFO mapreduce.Job:  map 100% reduce 84%\n",
      "17/01/31 01:25:19 INFO mapreduce.Job:  map 100% reduce 85%\n",
      "17/01/31 01:25:20 INFO mapreduce.Job:  map 100% reduce 86%\n",
      "17/01/31 01:25:22 INFO mapreduce.Job:  map 100% reduce 87%\n",
      "17/01/31 01:25:23 INFO mapreduce.Job:  map 100% reduce 88%\n",
      "17/01/31 01:25:25 INFO mapreduce.Job:  map 100% reduce 90%\n",
      "17/01/31 01:25:26 INFO mapreduce.Job:  map 100% reduce 91%\n",
      "17/01/31 01:25:28 INFO mapreduce.Job:  map 100% reduce 92%\n",
      "17/01/31 01:25:29 INFO mapreduce.Job:  map 100% reduce 93%\n",
      "17/01/31 01:25:31 INFO mapreduce.Job:  map 100% reduce 95%\n",
      "17/01/31 01:25:32 INFO mapreduce.Job:  map 100% reduce 96%\n",
      "17/01/31 01:25:35 INFO mapreduce.Job:  map 100% reduce 97%\n",
      "17/01/31 01:25:38 INFO mapreduce.Job:  map 100% reduce 98%\n",
      "17/01/31 01:25:41 INFO mapreduce.Job:  map 100% reduce 99%\n",
      "17/01/31 01:25:42 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/01/31 01:25:43 INFO mapreduce.Job: Job job_1484923209747_0364 completed successfully\n",
      "17/01/31 01:25:43 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=48086823\n",
      "\t\tFILE: Number of bytes written=96662468\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3462903\n",
      "\t\tHDFS: Number of bytes written=13316285\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=18702\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=107864\n",
      "\t\tTotal time spent by all map tasks (ms)=18702\n",
      "\t\tTotal time spent by all reduce tasks (ms)=107864\n",
      "\t\tTotal vcore-seconds taken by all map tasks=18702\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=107864\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=19150848\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=110452736\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=380821\n",
      "\t\tMap output bytes=47131413\n",
      "\t\tMap output materialized bytes=48086835\n",
      "\t\tInput split bytes=290\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=331775\n",
      "\t\tReduce shuffle bytes=48086835\n",
      "\t\tReduce input records=380821\n",
      "\t\tReduce output records=12012\n",
      "\t\tSpilled Records=761642\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=633\n",
      "\t\tCPU time spent (ms)=97180\n",
      "\t\tPhysical memory (bytes) snapshot=1234345984\n",
      "\t\tVirtual memory (bytes) snapshot=6236819456\n",
      "\t\tTotal committed heap usage (bytes)=1260912640\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tStripes analysis\n",
      "\t\tMapper=2\n",
      "\t\tReducer=2\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3462613\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=13316285\n",
      "17/01/31 01:25:43 INFO streaming.StreamJob: Output directory: /home/cloudera/W261/Spring2017/HW3/freq_stripes.3.5-output\n",
      "\n",
      "real\t1m25.158s\n",
      "user\t0m5.109s\n",
      "sys\t0m0.473s\n"
     ]
    }
   ],
   "source": [
    "# Hadoop command\n",
    "\n",
    "\n",
    "!hdfs dfs -rm -r /home/cloudera/W261/Spring2017/HW3/freq_stripes.3.5-output \n",
    "\n",
    "!time hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D stream.num.map.output.key.fields=3 \\\n",
    "    -D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "    -D mapreduce.partition.keycomparator.options=-k1,1 \\\n",
    "    -mapper /home/cloudera/W261/Spring2017/HW3/stripes_mapper35.py \\\n",
    "    -reducer /home/cloudera/W261/Spring2017/HW3/stripes_reducer35.py \\\n",
    "    -input   /home/cloudera/dfs/ProductPurchaseData/ProductPurchaseData.csv \\\n",
    "    -output /home/cloudera/W261/Spring2017/HW3/freq_stripes.3.5-output \\\n",
    "    -numReduceTasks 2 \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "    -cmdenv NUM_PARTITIONS=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stripes_mapper351.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stripes_mapper351.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import ast\n",
    "\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip().split('\\t',2)\n",
    "    key,pair_dict = line\n",
    "    pairs = ast.literal_eval(pair_dict)\n",
    "    # extract those pairs whose counts are over 100\n",
    "    frequent_pairs = dict((pair, count) for pair, count in pairs.items() if count > 100)\n",
    "    # and emit them\n",
    "    for frequent_pair,count in frequent_pairs.items():\n",
    "        print '{0}\\t{1}\\t{2}'.format(key,frequent_pair,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x stripes_mapper351.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /home/cloudera/W261/Spring2017/HW3/freq_stripes.3.51-output\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.9.1.jar] /tmp/streamjob4715335295154851299.jar tmpDir=null\n",
      "17/01/31 01:25:58 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/31 01:25:58 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "17/01/31 01:25:59 INFO mapred.FileInputFormat: Total input paths to process : 2\n",
      "17/01/31 01:25:59 WARN hdfs.DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1281)\n",
      "\tat java.lang.Thread.join(Thread.java:1355)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:950)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:688)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:877)\n",
      "17/01/31 01:25:59 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/01/31 01:25:59 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1484923209747_0365\n",
      "17/01/31 01:25:59 INFO impl.YarnClientImpl: Submitted application application_1484923209747_0365\n",
      "17/01/31 01:25:59 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1484923209747_0365/\n",
      "17/01/31 01:25:59 INFO mapreduce.Job: Running job: job_1484923209747_0365\n",
      "17/01/31 01:26:06 INFO mapreduce.Job: Job job_1484923209747_0365 running in uber mode : false\n",
      "17/01/31 01:26:06 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/01/31 01:26:16 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "17/01/31 01:26:17 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/01/31 01:26:22 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/01/31 01:26:23 INFO mapreduce.Job: Job job_1484923209747_0365 completed successfully\n",
      "17/01/31 01:26:23 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=32801\n",
      "\t\tFILE: Number of bytes written=432816\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=13316589\n",
      "\t\tHDFS: Number of bytes written=50868\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=15372\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4400\n",
      "\t\tTotal time spent by all map tasks (ms)=15372\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4400\n",
      "\t\tTotal vcore-seconds taken by all map tasks=15372\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4400\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=15740928\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4505600\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=12012\n",
      "\t\tMap output records=1312\n",
      "\t\tMap output bytes=30171\n",
      "\t\tMap output materialized bytes=32807\n",
      "\t\tInput split bytes=304\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1312\n",
      "\t\tReduce shuffle bytes=32807\n",
      "\t\tReduce input records=1312\n",
      "\t\tReduce output records=1311\n",
      "\t\tSpilled Records=2624\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=428\n",
      "\t\tCPU time spent (ms)=3030\n",
      "\t\tPhysical memory (bytes) snapshot=928337920\n",
      "\t\tVirtual memory (bytes) snapshot=4682629120\n",
      "\t\tTotal committed heap usage (bytes)=911736832\n",
      "\tPairs analysis\n",
      "\t\tReducer 2=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=13316285\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=50868\n",
      "17/01/31 01:26:23 INFO streaming.StreamJob: Output directory: /home/cloudera/W261/Spring2017/HW3/freq_stripes.3.51-output\n",
      "\n",
      "real\t0m27.465s\n",
      "user\t0m5.000s\n",
      "sys\t0m0.365s\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /home/cloudera/W261/Spring2017/HW3/freq_stripes.3.51-output \n",
    "\n",
    "!time hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "    -D stream.num.map.output.key.fields=3 \\\n",
    "    -D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "    -D mapreduce.partition.keycomparator.options=-k3,3nr \\\n",
    "    -mapper /home/cloudera/W261/Spring2017/HW3/stripes_mapper351.py \\\n",
    "    -reducer /home/cloudera/W261/Spring2017/HW3/pairs_reducer3.41.py \\\n",
    "    -input   /home/cloudera/W261/Spring2017/HW3/freq_stripes.3.5-output/* \\\n",
    "    -output /home/cloudera/W261/Spring2017/HW3/freq_stripes.3.51-output \\\n",
    "    -numReduceTasks 1 \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ---------------- top 50 frequent pairs --------------------------- #\n",
      "DAI62779\tELE17451\t1592\t0.0511880646925\n",
      "FRO40251\tSNA80324\t1412\t0.0454004694383\n",
      "DAI75645\tFRO40251\t1254\t0.0403202469374\n",
      "FRO40251\tGRO85051\t1213\t0.0390019613517\n",
      "DAI62779\tGRO73461\t1139\t0.0366226166361\n",
      "DAI75645\tSNA80324\t1130\t0.0363332368734\n",
      "DAI62779\tFRO40251\t1070\t0.0344040384554\n",
      "DAI62779\tSNA80324\t923\t0.0296775023311\n",
      "DAI62779\tDAI85309\t918\t0.0295167357963\n",
      "ELE32164\tGRO59710\t911\t0.0292916626475\n",
      "DAI62779\tDAI75645\t882\t0.0283592167454\n",
      "FRO40251\tGRO73461\t882\t0.0283592167454\n",
      "DAI62779\tELE92920\t877\t0.0281984502106\n",
      "FRO40251\tFRO92469\t835\t0.026848011318\n",
      "DAI62779\tELE32164\t832\t0.0267515513971\n",
      "DAI75645\tGRO73461\t712\t0.0228931545609\n",
      "DAI43223\tELE32164\t711\t0.022861001254\n",
      "DAI62779\tGRO30386\t709\t0.02279669464\n",
      "ELE17451\tFRO40251\t697\t0.0224108549564\n",
      "DAI85309\tELE99737\t659\t0.0211890292917\n",
      "DAI62779\tELE26917\t650\t0.020899649529\n",
      "GRO21487\tGRO73461\t631\t0.0202887366966\n",
      "DAI62779\tSNA45677\t604\t0.0194205974084\n",
      "ELE17451\tSNA80324\t597\t0.0191955242597\n",
      "DAI62779\tGRO71621\t595\t0.0191312176457\n",
      "DAI62779\tSNA55762\t593\t0.0190669110318\n",
      "DAI62779\tDAI83733\t586\t0.018841837883\n",
      "ELE17451\tGRO73461\t580\t0.0186489180412\n",
      "GRO73461\tSNA80324\t562\t0.0180701585158\n",
      "DAI62779\tGRO59710\t561\t0.0180380052088\n",
      "DAI62779\tFRO80039\t550\t0.0176843188322\n",
      "DAI75645\tELE17451\t547\t0.0175878589113\n",
      "DAI62779\tSNA93860\t537\t0.0172663258416\n",
      "DAI55148\tDAI62779\t526\t0.016912639465\n",
      "DAI43223\tGRO59710\t512\t0.0164624931674\n",
      "ELE17451\tELE32164\t511\t0.0164303398605\n",
      "DAI62779\tSNA18336\t506\t0.0162695733256\n",
      "ELE32164\tGRO73461\t486\t0.0156265071863\n",
      "DAI62779\tFRO78087\t482\t0.0154978939584\n",
      "DAI85309\tELE17451\t482\t0.0154978939584\n",
      "DAI62779\tGRO94758\t479\t0.0154014340375\n",
      "GRO85051\tSNA80324\t471\t0.0151442075817\n",
      "DAI62779\tGRO21487\t471\t0.0151442075817\n",
      "ELE17451\tGRO30386\t468\t0.0150477476608\n",
      "FRO85978\tSNA95666\t463\t0.014886981126\n",
      "DAI62779\tFRO19221\t462\t0.014854827819\n",
      "DAI62779\tGRO46854\t461\t0.0148226745121\n",
      "DAI43223\tDAI62779\t459\t0.0147583678981\n",
      "ELE92920\tSNA18336\t455\t0.0146297546703\n",
      "DAI88079\tFRO40251\t446\t0.0143403749076\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "!echo '# ---------------- top 50 frequent pairs --------------------------- #'\n",
    "!hdfs dfs -cat /home/cloudera/W261/Spring2017/HW3/freq_stripes.3.51-output/part-00000 | head -50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computational setup: single computer on Cloudera VM (CentOS 6) with two processors and 8 GB available to the VM, using two mappers and two reducers.\n",
    "\n",
    "Stripes: \n",
    "First step which merged and summed the stripes took 85 seconds. (2 reducers)\n",
    "Second step which calculated the relative frequencies and sorted the results took 27 seconds. (1 reducer)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python2]",
   "language": "python",
   "name": "conda-env-python2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
